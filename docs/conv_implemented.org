#+TITLE: 卷积网络实验
#+AUTHOR: stupid-coder
#+EMAIL: stupid-coder
#+STARTUP: indent
#+OPTIONS: H:2 ^:nil
#+INDEX: (mxnet)

#+BEGIN_QUOTE
本文用来说明一些常见卷积网络结构的实验,从而学习相关知识点.本文中代码都是在 cpu 上运行,所以网络结构都是简化版本.
#+END_QUOTE

* 卷积神经网络
  从 1998 年 LeNet 诞生,到 2012 年 AlexNet 在 ImageNet 分类比赛上大放异彩,使得卷积神经网络得到了新生.卷积神经网络的基础思想是利用图像天然的局部相关特性,使用局部链接提高模型对局部信息的敏感度,权值共享的方式减低模型参数,采样层来增强尺度不变性.采用反向传播算法自学习卷积核,从而避免了人工构造视觉特征需要大量专业知识的弊端.

  但是从头训练一个卷积神经网络模型需要大量的工程经验和模型训练经验.本文会在一个较为简单的数据集上尝试从头开始训练一个卷积神经网络,采用不同的网络模型(不同的网络结构&不同的激活函数等),不同的训练策略(学习率&初始化策略&优化算法).并希望能够在这个较小的数据集上达到绝对过拟合(训练集上准确性基本接近 100%),最后利用数据增强和正则化方法来降低模型过拟合,并结合模型集成相关技术,最终得到一个能够达到的最好模型.并且会尝试相关模型压缩等技术.

  本文会沿着如下路线进行说明和记录:
  + =基础网络= :: 首先尝试使用较为简单的 LeNet 作为卷积神经网络示例.
  + =学习率= :: 使用不同的学习率进行训练.
  + =激活函数= :: 采用 tanh, softplus, relu 的卷积神经网络.
  + =权值初始化= :: 采用 Xvair 的权值初始化.
  + =网络结构= :: AlexNet,VGG,NiN,GoogLeNet,ResNet,DenseNet.
  + =优化算法= :: sgd,adam
  + =归一化方法= :: batch-norm,layer-norm,instance-norm,group-norm.
  + =正则化方法= :: dropout,全局均值采样
  + =数据增强= :: 尝试常见的图像数据增强方法降低模型过拟合
  + =模型集成= :: 提高最终模型表现.
  + =模型压缩&剪枝= :: 模型压缩和剪枝相关尝试.

  最近在看李牧大神的<动手学深度学习>[fn:1],所以本文代码采用 mxnet 深度学习框架实现.

  评估标准除了传统的损失值和准确率.

* 数据集
首先介绍一下数据集,采用和 mxnet 示例中使用的相同图像分类数据集 Fashion-MNIST[fn:2],该数据集要比原始的 MNIST 要复杂一点,如[[图 1]]所示:

#+BEGIN_CENTER
#+NAME: 图 1
#+CAPTION: Fashion-MNIST 一些样本展示,每 3 行一个类别.
[[file:assets/fashion-mnist-sprite.png]]
#+END_CENTER

*mxnet* 中 *gluon.data.vision* 模块已经内置了该数据集,读取方式如下:
#+BEGIN_SRC python
  def data():
      return gdata.vision.FashionMNIST(train=True), gdata.vision.FashionMNIST(train=False)


  def dataset(batch_size=64):
      train_data, test_data = data()

      transformer = gdata.vision.transforms.Compose([
          gdata.vision.transforms.ToTensor()
          ])

      train_iter = gdata.DataLoader(train_data.transform_first(transformer),
                                    batch_size=batch_size, shuffle=True)

      test_iter = gdata.DataLoader(test_data.transform_first(transformer),
                                   batch_size=batch_size, shuffle=True)


      return train_iter, test_iter
#+END_SRC

标签数据为:
| 标签 | 描述        |
|------+-------------|
|    0 | T-shirt/top |
|    1 | Trouser     |
|    2 | Pullover    |
|    3 | Dress       |
|    4 | Coat        |
|    5 | Sandal      |
|    6 | Shirt       |
|    7 | Sneaker     |
|    8 | Bag         |
|    9 | Ankle boot  |

* LeNet 卷积神经网络
  卷积神经网络一般的网络结构为: [[conv]*+[pooling]*]* + [fc]*,即通过级联多组卷积层和采样层,然后级联一个或多个全链接层来实现.本文首先实现一个最为简单的卷积神经网络 LeNet[fn:3] 作为基础网络.

  LeNet 出现较早,为第一个实用的经典卷积神经网络模型.整个网络结构如[[图 2]]所示,由 2 层卷积层和 2 个最大值采样层交替组成,最后级联 3 层全链接层作分类.整个网络都是采用 5*5 卷积核,2*2 最大值采样层, sigmoid 作为激活函数.

  #+BEGIN_CENTER
  #+NAME: 图 2
  #+CAPTION: LeNet 网络结构.
  [[file:assets/LeNet.png]]
  #+END_CENTER

** LeNet 网络结构
采用 mxnet 构建网络结构,具体实现可以参考 mxnet 官网的教程.
#+BEGIN_SRC python
  def build_LeNet():
      net = nn.Sequential()
      net.add(
          nn.Conv2D(channels=6, kernel_size=5, strides=1, activation='sigmoid'),
          nn.MaxPool2D(pool_size=2, strides=2),
          nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),
          nn.MaxPool2D(pool_size=2, strides=2),
          nn.Dense(120, activation='sigmoid'),
          nn.Dense(84, activation='sigmoid'),
          nn.Dense(10)
      )
      net.initialize()
      return net
#+END_SRC

原论文中,最后一层的全链接层为采用颈向基函数来计算前一层输出 84 神经元与 [7*12] 的位图的欧式距离来进行对应的预测的(如[[图 3]]所示):
#+BEGIN_CENTER
#+NAME: 图 3
#+CAPTION: [7*12] 的位图.
[[file:assets/RBF_bitmap.png]]
#+END_CENTER

本文并不采用这种方法,采用交叉熵作为损失函数,SGD 算法进行优化.但是保持了 LeNet 的网络结构,中间全链接层仍然为 84 神经元.

输入数据为 [28,28,1] 的灰度图像.可以通过如下代码对网络进行预初始化,并打印出每层网络输出:
#+BEGIN_SRC python
  def pre_initialize_net(net):
      X = nd.random.uniform(shape=(1, 1, 28, 28)) # mxnet 中为[batch_size,channels,height,width]
      for layer in net:
          X = layer(X)
          print(layer.name, "output shape:\t", X.shape)
#+END_SRC

输出结果:
#+BEGIN_EXAMPLE
  conv0 output shape:	 (1, 6, 24, 24)
  pool0 output shape:	 (1, 6, 12, 12)
  conv1 output shape:	 (1, 16, 8, 8)
  pool1 output shape:	 (1, 16, 4, 4)
  dense0 output shape:	 (1, 120)
  dense1 output shape:	 (1, 84)
  dense2 output shape:	 (1, 10)
#+END_EXAMPLE

整个模型参数量为:
+ conv0: 6 * (5 * 5 * 1 + 1) = 156
+ conv1: 16 * (5 * 5 * 6 + 1) = 2416
+ dense0: 120 * (16 * 4 * 4 + 1) = 30840
+ dense1: 84 * (120 + 1) = 10164
+ dense2: 10 * (84 + 1) 850

总共参数量为: 44,426.

** 模型训练
损失函数采用交叉熵损失,优化器采用随机梯度下降(/SGD/).

#+BEGIN_SRC python
  def train(net, trainer, train_iter, test_iter, loss, num_epochs=5):
      train_ls = []
      train_acc = []
      test_ls = []
      test_acc = []
      for i in range(num_epochs):
          train_ls_sum, train_acc_sum = 0, 0
          begin_clock = time.clock()

          for X, y in train_iter:
              with autograd.record():
                  y_hat = net(X)
                  l = loss(y_hat, y).mean()
              l.backward()
              trainer.step(1)
              train_ls_sum += l.asscalar()
              train_acc_sum += accuracy(y_hat, y)

          train_ls.append(train_ls_sum/len(train_iter))
          train_acc.append(train_acc_sum/len(train_iter))
          tloss, tacc = evaluate(test_iter, net, loss)
          test_ls.append(tloss)
          test_acc.append(tacc)

          end_clock = time.clock()

          print("epoch {} - train loss: {}, train accuracy: {}, test loss: {}, test_accuracy: {}, cost time:{}".format(
              i+1, train_ls[-1], train_acc[-1], test_ls[-1], test_acc[-1], end_clock-begin_clock))
      return train_ls, train_acc, test_ls, test_acc


  def main(batch_size, lr):
      net = build_LeNet()
      describe_net(net)
      train_iter, test_iter = dataset(batch_size)
      trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':lr}) # SGD 优化器
      plot_loss_and_acc(train(net, trainer, train_iter, test_iter, gloss.SoftmaxCrossEntropyLoss()))
#+END_SRC

可以看到整个训练过程会将 batch 训练的损失值和准确值进行加和平均.然后在每次 epoch 之后计算在测试集上的损失值和准确性.对应的计算代码如下:

#+BEGIN_SRC python
  def accuracy(y_hat, y):
      return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()

  def evaluate(data_iter, net, loss_fn):
      acc = 0
      loss = 0
      for X,y in data_iter:
          y_hat = net(X)
          acc += accuracy(y_hat, y)
          loss += loss_fn(y_hat, y).mean().asscalar()
      return loss / len(data_iter), acc / len(data_iter)
#+END_SRC

** 实验结果
整个网络采用均值初始化,batch_size 设置为 256.观察效果.

在设定学习率为 0.1 时候,发现损失前期基本不下降.增大训练轮数发现,在很多轮之后,损失值和准确性有了很大的变化.如[[图 4]]所示:
#+BEGIN_CENTER
#+NAME: 图 4
#+CAPTION: 均值初始化,学习率设置为 0.1,100 轮训练结果.模型收敛速度较慢,经过 70 多个 epoch 之后才开始收敛.最终测试准确性只有 70%.
[[file:assets/acc_loss/lenet-uniform-01-1-101-73-loss-acc.png]]
#+END_CENTER

两种原因影响模型训练出现这种情况:
+ 学习率设置太低,这个很容易理解,学习率较低,模型对损失函数梯度更新不敏感.
+ 权值初始化对模型收敛也有较大影响,这个在后文中会介绍为什么权值初始化对深度神经网络的训练有着巨大的影响.

* 学习率

学习率主要应用在梯度下降算法中,用来调整一次权值更新的超参数.最简单的权值更新公式:
\begin{equation}
  \theta_{new} = \theta_{old} - \lambda \frac{\partial{J(\theta)}}{\partial{\theta}}(\theta_{old})
\notag
\end{equation}

学习率一般如何设置呢? 

其实在 LeCun 的早先一篇论文<effecient backprop>[fn:4]里有简单的介绍.基本思想就是说学习率不能设置过大,会导致模型不收敛,论文给出最优学习率公式.具体可以去看论文中说明.

学习率设置过大,有可能会带来损失值增大,模型不收敛;或者使得模型无法收敛到极小,损失无法达到最小.

学习率设置过小,损失值下降过慢,网络收敛速度慢,并且容易陷入局部极小,使得模型欠拟合,无法达到全局最优.
-----
#+BEGIN_CENTER
#+NAME: 图 5
#+CAPTION: 不同学习率对损失值的影响.可以看到较好的学习率应该可以使模型损失较快下降,并且最终损失值可以降到一个合理值.
[[file:assets/learning-rate-to-loss.jpg]]
#+END_CENTER

那么在深度神经网络中学习率如何设置呢?

LeCun 的论文中提出的最优学习率就无法直接计算,因为涉及到 Hessian 矩阵的逆.在参数量庞大的深度学习模型上,这个计算代价太大.所以大多数情况下,学习率是试出来的.但是也是有一些经验可寻的.比如说,[[http://cs231n.github.io/neural-networks-3][cs231n]]就有介绍说梯度更新比例经验值为 1e-3 附近.并且也给出了一些启发式的学习率调参方法.


#+BEGIN_QUOTE
学习率在训练过程中一般也不是一成不变的,一般常规做法是在网络前几轮训练采用较大的学习率,使得网络能够尽快开始收敛;随着训练过程,对学习率进行衰减,从而避免较大学习率无法收敛到极小的问题.
#+END_QUOTE

** 周期性学习率调整
可以想到的简单方法就是使用不同学习率在模型上简单训练几个 epoch,然后观察学习率和准确性的关系,从而确定一个学习率的大体值,在后许训练中采用学习率衰减方法.

论文<Cyclical Learning Rates for Training Neural Networks>[fn:5] 中给出了一个找寻学习率上下界的方法:在前几轮训练中,以使学习率从低到高线性增加,观察对应的准确性.将准确性开始提升的对应学习率设置为下界,将准确性开始变差或者开始上下波动的时候设置为上界.从而找到学习率的上下解.

周期性学习率调整代码如下:
#+BEGIN_SRC python
  def circle_learning_rate(iter_count, base_lr, max_lr, step_size):
      cycle = math.floor( 1 + iter_count / (2 * step_size) )
      ratio = abs( iter_count / step_size - 2 * cycle + 1 )
      return base_lr + (max_lr-base_lr) * max( (1-ratio), 0 )
#+END_SRC

其中,iter_count 为训练迭代次数,base_lr 和 max_lr 为周期性调整学习率的下界和上界,step_siz 为调整周期的一半迭代次数.学习率调整的变化图如下所示:
-----
#+BEGIN_CENTER
#+NAME: 图 6
#+CAPTION: 三角周期性调整习率策略.蓝色线表示学习率在上下学习率边界调整,step_size 为一个调整周期的一般迭代次数.
[[file:assets/circle-lr.png]]
#+END_CENTER


*** 周期性学习率上下界
论文中提出了通过在多个 epoch 中迭代递增学习率,然后通过观察学习率和损失值及准确性的关系确定最优学习率区间.将 loss 开始显著下降作为 base_lr,在 loss 开始进入平缓区或者有点上升时为 max_lr.

只需要利用上述的周期学习率调整,将 step_size 设置为最大迭代次数,就可以保证在训练周期中学习率递增.

采用 lenet 网络结构,8 个 epochs 训练周期.训练结果如下:
-----
#+BEGIN_CENTER
#+NAME: 图 7
#+CAPTION: LeNet 周期学习率测试.可以确定学习率设置为 base_lr=0.015,max_lr=0.06.在 0.015 时,loss 开始下降,在 0.06 的时候 loss 开始进入平缓,有一些上升.表示学习率有点偏大了.
[[file:assets/lenet_circle_lr_test.png]]
#+END_CENTER

*** 周期性学习率训练结果
基于上述确定的学习率上下界 base_lr=0.015 和 max_lr=0.06,设置 step_size 为,采用周期性学习率调整算法进行训练 300 轮.结果如下:
-----
#+NAME: 图 8
#+CAPTION: batch_size=256, base_lr=0.015, max_lr=0.06, step_size=8*256 周期调整学习率方法.最终准确性为 78%.
[[file:assets/acc_loss/lenet-uniform-circle-1-301-78-loss-acc.png]]

效果并不好,比固定学习率 0.1 花了更长时间损失值才开始明显下降.很明显学习率设置还是太低了.后续在做实验.

** 衰减学习
在[[http://cs231n.github.io/neural-networks-3/][cs231n]]中介绍了多种递减学习率的方法,例如如下:
+ =步衰减= :: 几轮 epoch 对学习率进行衰减,例如 5 个 epcoh 衰减成 0.5,或者 10 个 epoch 衰减到 0.1.一个启发式方法是通过观察验证集上准确性一旦保持不变,那么就对学习率衰减 0.5.
+ =指数衰减= :: 具有公式 $\alpha=\alpha_{0}e^{-kt}$,其中 $\alpha_{0},k$ 为超参,t 为训练次数.
+ =1/t 衰减= :: 具有公式 $\alpha=\alpha_{0}/(1+kt)$,其中 $\alpha_{0},k$ 为超参,t 为训练次数.

*** 步衰减
初始学习率设置为 1.0,采用 10 个 epoch 衰减 0.5 的策略,训练 100 轮结果如下:
-----
#+BEGIN_CENTER
#+NAME: 图 9
#+CAPTION: 可以看到前期由于采用 1.0 学习率,在经过 10 个左右 epoch 后,损失值就开始下降.最终准确性达到 0.83.
[[file:assets/acc_loss/lenet-uniform-10-05-decay-loss-acc.png]]
#+END_CENTER

* 调整初始化策略
不管是从[[图 4]]还是从[[图 8]]都可以看到在训练的前期损失值都有一个不下将的区间,该区间的长短由学习率的大小决定;大的学习率可以很快的进入损失值下降的区间;小的学习率需要较长时间来进入损失值下降区间.直观上将该区间内,模型在一层层初始化网络权值,学习率大初始化的快.那么有没有其他方法可以加快初始化过程呢.这里就要用到一些权值初始化方法.

** Xavier 初始化策略
Glorot 在论文<Understanding the diffculty of training deep feedforward neural networks> 通过观察不同激活函数和不同初始化策略下激活值的分布提出了叫做 Xavier 的权值初始化策略.论文中采用了均值采样 $U\left[-\frac{1}{\sqrt{n}},\frac{1}{\sqrt{n}}\right]$ 作为实验初始化策略,其中 n 为输入的神经元个数.

*** sigmoid 作为激活函数
首先论文观察 4 层采用 sigmoid 作为激活函数的激活值分布:
-----
#+BEGIN_CENTER
#+NAME: 图 9
#+CAPTION: 采用 sigmoid 作为激活函数,均值初始化的 4 层神经网络激活值分布(均值和方差).可以看到高层迅速进入饱和状态,在 100epoch 之后慢慢变成非饱和状态.
[[file:assets/activation-sigmoid-4.png]]
#+END_CENTER

分析: $sigmoid(b+Wh)$ 由于结合均值随机初始化,在训练前期低层输入隐特征 h 变化可能和分类目标没有那么具有关联,所以 softmax 分类层更倾向于学习不同分类的偏置,而将 Wh 设置为 0.反向传播过程中,$\frac{\partial{L}}{\patial{h}}$ 更倾向为 0,在用 sigmoid 作为激活函数的时候,会使得激活函数陷入饱和状态,从而使得反向传播错误无法向前传播,从而使得低层网络层无法学习到有用信息.最终缓慢的,当低网络层学习到更为有用的信息后,高网络逃出饱和状态.整个网络层随着低网络层进入饱和状态而稳定.sigmoid 的网络一般性能都较差(即泛化能力不足).

*** tanh 作为激活函数
介于上述情况,由于高网络层激活值容易首先进入 0 值范围.那么采用以 0 为均值的对称激活函数可以规避陷入饱和状态.采用均值采样观察到了饱和状态出现在了第一层,然后依此传播到高层.

-----
#+BEGIN_CENTER
#+NAME: 图 10
#+CAPTION: 98 百分位数(标记)和标准差(实线+标记).可以看到第一层先进入饱和,然后第二层,第三层.
[[file:assets/activation-tanh.png]]
#+END_CENTER

*** Softsign 作为激活函数
Softsign 激活函数 $x/(1+|x|)$ 和 tanh 激活函数很相似.除了饱和状态为平滑渐进线(多项式饱和和指数).

-----
#+BEGIN_CENTER
#+NAME: 图 11
#+CAPTION: 98 百分位数(标记)和标准差(实线+标记).可以 softsign 作为激活函数时,所有层一起慢慢进入饱和状态.
[[file:assets/activation-softsign.png]]
#+END_CENTER


并且 tanh 激活函数和 softsign 激活函数训练出的模型最后的激活值分布也是不同.tanh 的激活值分布在(-1,1)和 0 值附近.而 softsign 的激活值分布在 0 点和 0 点到(-1,1)之间的区域.
-----
#+BEGIN_CENTER
#+NAME: 图 12
#+CAPTION: (上图):为 tanh 作为激活函数的模型输出的激活值分布,可以看到低层激活值更多集中在-1 和 1 的饱和区域.(下图):为 softsign 的激活函数的模型输出的激活值分布,可以看到激活值大多数集中在(-0.6,-0.8)和(0.6,0.8)之间,没有饱和,并且是非线性区域.
[[file:assets/activation-value-of-tanh-and-sfotsign.png]]
#+END_CENTER

*** 损失函数
论文通过可视化一个单层隐层的网络,tanh 作为激活值,讨论了对数似然损失函数 $-\log{P(y|x)}$ 和 二次损失函数.下图显示二次损失函数具有更多的稳定平面.
-----
#+BEGIN_CENTER
#+NAME: 图 13
#+CAPTION: 黑色为交叉熵损失,红色为二次损失.
[[file:assets/loss_function.png]]
#+END_CENTER

*** Xavier 初始化策略
通过观察初始化状态下的梯度传播过程,优化初始化策略对梯度传播效率.

假设:全链接神经网络的激活函数为对称,并且在 0 点具有单位梯度(即 $f^{'}(0)=1$).记 $z^{i}$ 为 i 网络层的激活向量; $s^{i}$ 为 i 网络层的激活函数参数向量.所以, $s^{i}=z^{i}W^{i}+b^{i}$ 和 $z_{i+1}=f(s^{i})$,定义梯度为:
\begin{equation}
  \frac{\partial{Cost}}{\partial{s_{k}^{i}}} = f'(s_{k}^{i})W_{k}^{i+1}\frac{\partial{Cost}}{\partial{s^{i+1}}} \\
  \frac{\partial{Cost}}{\partial{w_{l,k}^{i}}} = z_{l}^{i}\frac{\partial{Cost}}{\partial{s_{k}^{i}}}
\notag
\end{equation}

激活值的方差可以表示为输入数据方差和初始权重的方差线性组合(由于激活值在 0 值附近,且梯度为 1,所以 f(x)=x):
\begin{equation}
  f'(s_{k}^{i}) = 1 \\
  Var[z^{i}] = Var[x]\prod_{i'=0}^{i-1}n_{i'}Var[W^{i'}]
\notag
\end{equation}

其中 $Var[W^{i'}]$ 为第 $i'$ 网络层的初始化权重共享的方差, $n^{i'}$ 为第 i^{i} 网络层的权重数量.对于一个具有 d 层的网络来说:
\begin{aligned}
  Var[\frac{\partial{Cost}}{\partial{s^{i}}}] &= Var[\frac{\partial{Cost}}{\partial{s^{d}}}]\prod_{i'=i}^{d}n_{i'+1}Var[W^{i'}] \\
    Var\left[\frac{\partial{Cost}}{\partial{w^{i}}}\right] &= \prod_{i'=0}^{i-1}n_{i'}Var[W^{i'}]\prod_{i'=i}^{d-1}n_{i'+1}Var[W^{i'}] \\
    & \times Var[x]Var[\frac{\partial{Cost}}{\partial{s^{d}}}]
\notag
\end{aligned}

从向前传播角度看,希望激活值的方差不变,则: $\forall{(i,i')},Var[z^{i}]=Var[z^{i'}]$.

从反向传播角度看,希望对于激活前的值方差不变: $\forall{(i,i')},Var[\frac{\partial{Cost}}{\partial{s^{i}}}]=Var[\frac{\partial{Cost}}{\partial{s^{i'}}}]$

要满足上述两种约束,只需要:
\begin{equation}
  \forall{i}, n_{i}Var[W^{i}] = 1 \\
  \forall{i}, n_{i+1}Var[W^{i}] = 1 
  \notag
\end{equation}

结合起来就是: $\forall{i}, Var[W^{i}]=\frac{2}{n_{i}+n_{i+1}}$. 当所有网络层具有相同神经元则满足.

结合均值随机初始化,提出 *Xvaier* 初始化方法:
\begin{equation}
  W \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_{j}+n_{j+1}}},\frac{\sqrt{6}}{\sqrt{n_{j}+n_{j+1}}}\right]
\notag
\end{equation}

*** 实验结果

LeNet 采用 sigmoid 作为激活函数,Xavier 作为初始化策略,学习率为 0.1,10 轮训练结果如下:
-----
#+BEGIN_CENTER
#+CAPTION: 网络从 4 epoch 开始收敛(均值初始化网络需要 70 个 epoch 才开始收敛).测试集上准确性达到了 86%.
[[file:assets/acc_loss/lenet-xavier-sigmoid-01-1-101-86-loss-acc.png]]
#+END_CENTER


LeNet 采用 tanh 作为激活函数,Xavier 作为初始化策略,学习率为 0.1,10 轮训练结果如下:
-----
#+BEGIN_CENTER
#+CAPTION: 网络第一轮就开始开始收敛.最终模型在测试集上准确性达到了 89%,并有过拟合现象.
[[file:assets/acc_loss/lenet-xavier-tanh-01-1-101-89-loss-acc.png]]
#+END_CENTER


** TODO He 初始化

* 激活函数

激活函数主要是用来在深度神经网络中引入未线性变换,从而提高网络的拟合能力.激活函数在反向传播的时候,同时需要保证不要轻易出现饱和现象,从而使得梯度无法传播.不同的激活函数在不同的网络结构中使用,使用的不得当会影响网络的性能和收敛性.

** sigmoid
激活函数公式:
\begin{equation}
  sigmoid(x) = \frac{1}{1+e^{-x}} 
  \notag
\end{equation}

梯度公式:
\begin{equation}
  sigmoid'(x) = sigmoid(x)(1-sigmoid(x))
\notag
\end{equation}

激活函数图像和对应的梯度图如下:
-----
#+BEGIN_CENTER
#+CAPTION: sigmoid 的函数图像和梯度图像. 
[[file:assets/activation/sigmoid.png]]
#+END_CENTER

经过 sigmoid 激活后的值都是大于 0 的,所以均值为正数.并且可以看到梯度最大值为 0.25,所以较为容易陷入饱和状态.一般只有在特殊时候使用,例如 LSTM 网络中的各个门.

** tanh
激活函数公式:
\begin{equation}
  \tanh{(x)} = \frac{\sinh{(x)}}{\cosh{(x)})} = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
  \notag
\end{equation}

梯度公式:
\begin{equation}
  \tanh'{(x)} = 1 - \tanh^{2}{(x)}
\notag
\end{equation}

#+BEGIN_CENTER
#+CAPTION: tanh 的函数图像和梯度图像.
[[file:assets/activation/tanh.png]]
#+END_CENTER

对称的激活函数,均值为 0.优先在输入归一化的网络中.并且作为激活函数的网络收敛速度要快于 sigmoid 函数.同时看到 tanh 函数的不饱和区域很小[-2.5,+2.5],其他区域的梯度基本都接近 0 了.有时候可以通过添加一个较小的线性元素来避免这些平坦区域 $f(x)=tanh(x)+ax$.


** softsign
激活函数公式:
\begin{equation}
  signsoft(x) = \frac{x}{1-|x|}
  \notag
\end{equation}

梯度公式:
\begin{equation}
  signsoft'(x) = \frac{1}{(1+|x|)^{2}}
\notag
\end{equation}

-----
#+BEGIN_CENTER
#+CAPTION: softsign 的函数图像和梯度图像.
[[file:assets/activation/softsign.png]]
#+END_CENTER

可以看到激活函数的不饱和区域要比 tanh 要大,能够学习到更多信息.

** relu
激活函数公式:
\begin{equation}
  relu(x) = max(0, x)
  \notag
\end{equation}

梯度公式:
\begin{equation}
  relu'(x) = \begin{cases}
    1 & x > 0 \\
    0 & x < 0
    \end{cases}
  \notag
\end{equation}

-----
#+BEGIN_CENTER
#+CAPTION: relu 的函数图像和梯度图像.
[[file:assets/activation/relu.png]]
#+END_CENTER

该激活函数计算更为简单,并且在大于 0 的区域,梯度为 1,保证反向传播时,误差可以传播.

* 网络结构
LeNet 之后最为成功的卷积神经网络是 AlexNet,该网络在 ImageNet2012 比赛上大放异彩,从而使得卷积神经网络又重新走入视野之中.

** AlexNet
LeNet 提出后,卷积神经网络沉寂了很多.90 年代完全被支持向量机统治.主要原因是一方面,神经网络计算复杂,当时没有对应的 GPU 加速器,所以需要花费很久时间来训练一个现在看来并不大的神经网络.另一方面,当时也没有大量研究参数初始化和非凸优化等领域,导致复杂的神经网络也非常难以训练.

在卷积神经网络出现之前,机器视觉主要依赖于具有大量相关经验的学者研究出的人工特征,例如(SIFT,HOG),然后将这些人工构造的特征喂给机器学习来进行学习.随着卷积神经网络的大范围应用,可以预见到的是卷积神经网络就像这些有经验的学者一样通过数据来学习对应的特征表达.一层一层,从而实现最终的分类任务.由于这些特征完全是与图像和任务有关,所以提取的特征表达效率和效果都要好于人工特征.

*** 网络结构
AlexNet 的网络结构基本和 LeNet 的结构相似,不过也有一些显著区别:
+ =AlexNet 采用更深的卷积神经网络= :: 包括 8 层变换,其中包括 5 层卷积层,2 层全链接隐含层以及 1 层全链接输出层.
+ =激活函数= :: 采用 relu 作为激活函数,在正区间 relu 梯度的恒等为 1.因此不会出现因为初始化不当,使得模型无法训练.
+ =dropout= :: 采用丢弃法来控制全链接层的复杂度.
+ =数据增强= :: 采用了大量的数据增强方法,扩大数据集,从而避免过拟合.


-----
#+BEGIN_CENTER
#+NAME: AlexNet
#+CAPTION: AlexNet 网络结构.输入为 224*224 的 3 通道图像,第一层为 11*11,步长为 4 的卷积层,跟着一个 3*3,步长为 2 的最大值采样层;第二层为 5*5,步长为 1 的卷积层,跟着一个 3*3,步长为 2 的最大值采样层;第三层为 3*3,步长为 1 的卷积层;第四层为 3*3,步长为 1 的卷积层;第五层为 3*3,步长为 1 的卷积层,然后跟着一个 3*3,步长为 2 的最大值采样层.随后是两层 4096 全链接隐含层.最后为 1000 路 softmax 层.
[[file:assets/AlexNet.png]]
#+END_CENTER

对应代码如下:
#+BEGIN_SRC python
  def build_AlexNet(restore_dir):
      network = nn.Sequential()
      network.add(
          nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),
          nn.MaxPool2D(pool_size=3, strides=2),
          nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),
          nn.MaxPool2D(pool_size=3, strides=2),
          nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
          nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
          nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),
          nn.MaxPool2D(pool_size=3, strides=2),
          nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
          nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
          nn.Dense(10)
      )

      if restore_dir:
          helper.restore(network, restore_dir)
      else:
          network.initialize(ctx=helper.ctx, init=init.Xavier())

      return network
#+END_SRC

原始论文中说输入的图像为 224*224 的,但是其实如果输入为 227*227,整个网络结构和输出的特征图尺度就对了,结果如下:
#+BEGIN_EXAMPLE
INFO:helper:conv0 - output shape:(1L, 96L, 55L, 55L)
INFO:helper:pool0 - output shape:(1L, 96L, 27L, 27L)
INFO:helper:conv1 - output shape:(1L, 256L, 27L, 27L)
INFO:helper:pool1 - output shape:(1L, 256L, 13L, 13L)
INFO:helper:conv2 - output shape:(1L, 384L, 13L, 13L)
INFO:helper:conv3 - output shape:(1L, 384L, 13L, 13L)
INFO:helper:conv4 - output shape:(1L, 256L, 13L, 13L)
INFO:helper:pool2 - output shape:(1L, 256L, 6L, 6L)
INFO:helper:dense0 - output shape:(1L, 4096L)
INFO:helper:dropout0 - output shape:(1L, 4096L)
INFO:helper:dense1 - output shape:(1L, 4096L)
INFO:helper:dropout1 - output shape:(1L, 4096L)
INFO:helper:dense2 - output shape:(1L, 10L)
#+END_EXAMPLE

整个网络的参数量:
| 网络层 | 参数量                    |
| conv0  | 96*(11*11*3+1)=34944      |
| conv1  | 256*(5*5*96+1)=614656     |
| conv2  | 384*(3*3*256+1)=885120    |
| conv3  | 384*(3*3*384+1)=1327488   |
| conv4  | 256*(3*3*384+1)=884992    |
| dense0 | 4096*(256*6*6+1)=37752832 |
| dense1 | 4096*(4096+1)=16781312    |
| dense2 | 10*(4096+1)=40970         |
| total  | 58,322,314=58M            |

*** 实验结果
以 batch_size=256, learning_rate=0.1, xavier 作为初始化策略.结果如下:
-----
#+BEGIN_CENTER
#+NAME: AlexNet 试验结果
#+CAPTION: 实验参数:batch_size=256,learning_rate=0.1, xavier 初始化策略.
[[file:assets/acc_loss/alexnet-xavier-01-1-101-93-acc-loss.png]]
#+END_CENTER

可以看到虽然采用了 dropout 方法来抑制过拟合,但是由于该网络结构是为了 ImageNet 这种大数据研发的,所以在 Fashion-MNIST 上还是过于复杂了.

学习率还是应该再降低一些.后续可以做一下实验.

** VGGNet
VGGNet 为 ImageNet 2014 年的冠军网络.VGGNet 的网络改进就是加深网络层数来增强网络的非线性拟合能力,并且重复使用小尺度卷积提取体征.

*** 网络结构
VGGNet 相对于 AlexNet 进行了如下的改动:
+ 通过堆叠多个小感受野的 3*3 卷积核来获取大感受野(例如 2 层 3*3 卷积层的感受野为 5,但是额外增加了非线性能力;并且 2 层 3*3 卷积层的参数数量只有 $2*(3^{2}C^{2})=18C^{2}$,5*5 卷积层参数数量为 $5*5C^{2}=25C^{2}$).并且卷积核的步长都是 1,增加填充 1,保证卷积的空间尺度不变.
+ 引入了 1*1 卷积核,来增加通道之间的非线性变换.
+ 只采用 2*2 采样尺度,步长为 2 的最大值采样层.


-----
#+BEGIN_CENTER
#+NAME: VGGNet 网络结构
#+CAPTION: VGGNet 网络结构,根据不同卷积配置总共有 6 种 VGGNet(A-E)
[[file:assets/VGGNet.png]]
#+END_CENTER

可以看到只采用了 3*3 卷积层,每组卷积层(最大值采样层之间的卷积层)具有相同的通道数量.

参数数量:
| Network              | A,A-LRN |   B |   C |   D |   E |
| Num of parameters(M) |     133 | 133 | 134 | 138 | 144 |

对应代码如下:
#+BEGIN_SRC python
  def build_VGGNet(restore_dir):

      def vgg_block(num_convs, num_channels):
          blk = nn.Sequential()
          for _ in range(num_convs):
              blk.add(nn.Conv2D(num_channels, kernel_size=3,
                                padding=1, activation='relu'))
          blk.add(nn.MaxPool2D(pool_size=2, strides=2))
          return blk

      network = nn.Sequential()

      conv_arch = ((1,64), (1, 128), (2, 256), (2, 512), (2, 512))  # VGG11

      for (num_convs, num_channels) in conv_arch:
          net.add(vgg_block(num_convs, num_channels))

      net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
              nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
              nn.Dense(10))

      if restore_dir:
          helper.restore(network, restore_dir)
      else:
          network.initialize(ctx=helper.ctx, init=init.Xavier())

      return network
#+END_SRC

*** 实验结果
由于 VGGNet 网络层非常深(11 层),所以正向传播和反向传播都需要存储非常多的.又由于数据集较小,所以不需要这么多通道,将所有通道降为 0.25,但是保持住所有网络层深度.

-----
#+BEGIN_CENTER
#+NAME: VGGNet 实验结果
#+CAPTION: VGGNet 试验结果,由于资源问题,只训练了 5 轮.最后的准确性达到了 86%.
[[file:assets/acc_loss/vggnet-xavier-001-1-5-86-acc-loss.png]]
#+END_CENTER

** NiN
Network in Network[fn:6]的构建网络的思路串联多个卷积层和全链接层构建一个小网络作为整个网络构建的一个模块,利用这个模块构建深层网络.

*** 网络结构
NiN 网络提出传统的卷积层只是一个简单的线性分类网络,需要输入特征具有良好的线性可分性才能达到较好的分类效果.NiN 网络提出在卷积层之间增加全链接层来提高网络的非线型分类能力,
+ NiN 利用 1*1 卷积层来替代全链接层,可以理解是在卷积层之间增加了 1*1 卷积层来构建网络.增加卷积层通道数据之间的非线性变化能力.
+ 通过增加网络的非线性分类能力,将最后一层卷积层输出通道数对应于分类置信值,采用全局均值采样层代替分类输出层,提高了分类泛化能力.


-----
#+BEGIN_CENTER
#+NAME: NiN 网络结构
#+CAPTION:  NiN 网络结构.在网络层之间添加 1*1 卷积层来提高通道特征交叉能力.
[[file:assets/NiNNetwork.png]]
#+END_CENTER

NiN 网络的卷积层感受野和 AlexNet 前 3 层一致.对应代码如下:
#+BEGIN_SRC python
  def build_nin(restore_dir):

      def nin_block(num_channels, kernel_size, strides, padding):
          blk = nn.Sequential()
          blk.add(nn.Conv2D(num_channels, kernel_size,
                            strides, padding, activation='relu'),
                  nn.Conv2D(num_channels, kernel_size=1, activation='relu'),
                  nn.Conv2D(num_chanenls, kernel_size=1, activation='relu'))
          return blk

      network = nn.Sequential()
      network.add(
          nin_block(96, kernel_size=11, strides=4, padding=0),
          nn.MaxPool2D(pool_size=3, strides=2),
          nin_block(256, kernel_size=5, strides=1, padding=2),
          nn.MaxPool2D(pool_size=3, strides=2),
          nin_block(384, kernel_size=3, strides=1, padding=1),
          nn.MaxPool2D(pool_size=3, strides=2),
          nin_block(10, kernel_size=3, strides=1, padding=1),
          nn.GlobalAvgPool2D(),
          nn.Flatten())

      if restore_dir:
          helper.restore(network, restore_dir)
      else:
          network.initialize(ctx=helper.ctx)

      return network
#+END_SRC

*** 实验结果
NiN 一般需要选取较大的学习率,但是为了对比,所以还是沿用 0.01.结果如下:
-----
#+BEGIN_CENTER
#+NAME: NiN 实验结果
#+CAPTION: NiN 学习率 0.01, 5 轮训练结果.
[[file:assets/acc_loss/nin-xavier-1-5-61-acc-loss.png]]
#+END_CENTER

看起来效果要比 VGG 差很多,应该使用更大的学习率.

** GoogLeNet
和 VGG 同年出现的 GoogLeNet[fn:7]在 ImageNet 2014 年比赛中获得了冠军网络,虽然从名字上向 LeNet 致敬,但是和 LeNet 网络完全不同.本且 GoogLeNet 具有很多版本,这里只按照和 mxnet 教程一样的,介绍第一个版本.

*** Inception 
GoogLeNet 中的基础卷积块为 Inception 块.该卷积块要比 NiN 更为复杂.

-----
#+BEGIN_CENTER
#+NAME: inception-block
#+CAPTION: (A)为 inception 块的基础版本;(B)加上通道降低的 inception.
[[file:assets/inception.png]]
#+END_CENTER

从上图可以看到一个 inception 模块具有四条并行线路.前三条为 1*1,3*3,5*5 卷积层来处理不同尺度的特征.3*3 和 5*5 的卷积执行之前有 1*1 卷积用来减少通道数量,从而减低模型的复杂度.第四条线路采用 3*3 最大池化层,后接 1*1 卷积层来增加通道间信息交互.其中这四条路线都采用对应的填充来保证输入和输出的空间维度(高宽)一致,最后将结果在通道维度上进行拼接输出.

对应的代码实现:
#+BEGIN_SRC python
  class Inception(nn.Block):
      def __init__(self, c1, c2, c3, c4, **kwargs):
          super(Inception, self).__init__(**kwargs)

          self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')

          self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')
          self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1, activation='relu')

          self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')
          self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2, activation='relu')

          self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)
          self.p4_2 = nn.Conv2D(c4[0], kernel_size=1, activation='relu')


      def forward(self, x):
          p1 = self.p1_1(x)
          p2 = self.p2_2(self.p2_1(x))
          p3 = self.p3_2(self.p3_1(x))
          p4 = self.p4_2(self.p4_1(x))
          return nd.concat(p1, p2, p3, p4, dim=1)
#+END_SRC

*** 网络结构
GoogLeNet 和 VGG 的整体结构类似,主体卷积部分为 5 个模块组成,每个模块之间由步长为 2 的,3*3 最大值采样来减少空间维度.

+ =第一个卷积模块= :: 和 ZFNet 类似,首先采用 7*7,步长为 2 的卷积层在原始输入空间提取空间尺度较大的特征,然后接着最大值采样,降低整个模型的空间尺度.
               #+BEGIN_SRC python
                 def block1():
                         b = nn.Sequential()
                         b.add( nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),
                                nn.MaxPool2D(pool_size=3, strides=2, padding=1))
                         return b
               #+END_SRC
+ =第二个卷积模块= :: 首先 1*1 卷积核进行通道间变换,然后执行扩大 3 倍通道数的 3*3 卷积操作.
               #+BEGIN_SRC python
                 def block2():
                         b = nn.Sequential()
                         b.add( nn.Conv2D(64, kernel_size=1, activation='relu'),
                                nn.Conv2D(192, kernel_size=3, padding=1),
                                nn.MaxPool2D(pool_size=3, strides=2, padding=1))
                         return b
               #+END_SRC
+ =第三个卷积模块= :: 由两个 inception 模块构成.第一个 inception 模块的输出通道数为 64+128+32+32=256,四条路线的通道数比例为 2:4:1:1.第二条和第三条线路会先对输入的通道数进行压缩 96/192=1/2 和 16/192=1/12.第二层 inception 模块输出通道增到 128+192+96+64=480,每条线路的输出通道数比例为:128:192:96:64=4:6:3:2.第二条和第三条线路分别对输入通道数减少到 128/256=1/2 和 32/256=1/8.
               #+BEGIN_SRC python
                     def block3():
                         b = nn.Sequential()
                         b.add( Inception(64, (96, 128), (16, 32), 32),
                                Inception(128, (128, 192), (32, 96), 64),
                                nn.MaxPool2D(pool_size=3, strides=2, padding=1))
                         return b
               #+END_SRC
+ =第四个卷积模块= :: 由五个 inception 模块构成.输出通道数分别为 192+208+48+64=512, 160+224+64+64=512, 128+256+64+64=512, 112+288+64+64=528 和 256+320+128+128=832.各个模块中通道数压缩比例都不同.
               #+BEGIN_SRC python
                 def block4():
                         b = nn.Sequential()
                         b.add( Inception(192, (96, 208), (16, 48), 64),
                                Inception(160, (112, 224), (24, 64), 64),
                                Inception(128, (128, 256), (24, 64), 64),
                                Inception(112, (144, 288), (32, 64), 64),
                                Inception(256, (160, 320), (32, 128), 128),
                                nn.MaxPool2D(pool_size=3, strides=2, padding=1))
                         return b
               #+END_SRC
+ =第五个卷积模块= :: 由两个 inception 模块构成.输出通道数为 256+320+128+128=832 和 384+384+128+128=1024.
               #+BEGIN_SRC python
                 def block5():
                     b = nn.Sequential()
                     b.add( Inception(256, (160, 320), (32, 128), 128),
                            Inception(384, (192, 384), (48, 128), 128),
                            nn.GlobalAvgPool2D())
                     return b
               #+END_SRC
+ =输出层= :: 全局均值层之后跟着全链接层来进行分类.因为全局均值采样层存在,所有任意尺度的图像最后都具有相同的通道数,所以整个网络层都可以处理任意尺度的图像.
           #+BEGIN_SRC python
             network = nn.Sequential()

             network.add(
                 block1(),
                 block2(),
                 block3(),
                 block4(),
                 block5(),
                 nn.Dense(10))
           #+END_SRC


-----
#+BEGIN_CENTER
#+NAME: GoogleNet 网络结构
[[file:assets/GoogleNet-Arch.png]]
#+END_CENTER

*** 实验结果
由于整个 GoogLeNet 计算较为复杂,所以训练采用的图像为 96*96.学习率设置为 0.1.

-----
#+BEGIN_CENTER
#+NAME: GoogLeNet 实验结果
#+CAPTION: 学习率设置为 0.1,输入图像为 96*96.
[[file:assets/acc_loss/googlenet-xavier-01-1-5-86-acc-loss.png]]
#+END_CENTER

** ResNet
何恺明大神在 2015 年提出了残差网络(ResNet)[fn:9][fn:10],主要是通过添加夸层链接,使得网络来拟合更容易拟合的残差来增加网络的深度.从反向传播的角度来看,添加的夸层链接给予了反向传播算法一个夸层的梯度反向传播路径,从而使得梯度能够传播的更深.

*** 残差层
假设输入的为 x,我们需要学习的函数映射为 $f(x)$,那么残差块只需要拟合出残差映射 $f(x)-x$.并且残差映射实际上更容易优化.下图就是 ResNet 中的基础块,即残差块(/residual block/).
-----
#+BEGIN_CENTER
#+NAME: residual-block
#+CAPTION: 残差块.一是输入数据可以夸层跳跃,残差更容易拟合,其次反向传播的梯度也可以通过夸层链接快速传递.
[[file:assets/residual-block.png]]
#+END_CENTER

ResNet 和 VGGNet 一样,全部使用 3*3 卷积层设计.残差块中首先有两个同样输出通道数的 3*3 卷积层.每个卷积层后接一个批量归一化层和 ReLU 激活函数.然后将输入跳过这两个卷积层直接加在最后的 ReLU 激活函数之前.这样需要两个卷积层的输出和输入形状一样.如果想改变通道数,可以通过额外的 1*1 卷积层来将输入变换成需要的形状.

具体实现如下:
#+BEGIN_SRC python
  class Residual(nn.Block):
      def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):
          super(Residual, self).__init__(**kwargs)
          self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1, strides=strides)
          self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)

          if use_1x1conv:
              self.conv3 = nn.Conv2D(num_channels, kernel_size=1, strides=strides)
          else:
              self.conv3 = None

          self.bn1 = nn.BatchNorm()
          self.bn2 = nn.BatchNorm()

      def forward(self, X):
          Y = nd.relu(self.bn1(self.conv1(X)))
          Y = self.bn2(self.conv2(Y))
          if self.conv3:
              X = self.conv3(X)
          return nd.relu(Y+X)
#+END_SRC

*** 网络结构
ResNet 的前两层和 GoogLeNet 一样:在输出的通道数为 64 的,步长为 2 的 7*7 卷积层之后跟着步长为 2 的 3*3 最大值采样层.不同之处在于 ResNet 每个卷积层后增加批量归一化层.

GoogLeNet 之后跟着 4 个由多个 Inception 组成的模块.ResNet 则使用 4 个由残差块组成的模块,每个模块使用若干个同样输出通道数的残差块.由于之前已经采用了步长为 2 的最大值采样,所以第一个模块的通道数和输入通道数一致,无需减少高和宽.之后的模块在第一个残差块里将上一模块的通道数翻倍,并减半高和宽.

#+BEGIN_SRC python
  def build_resnet(restore_dir):

      def resnet_block(num_channels, num_residuals, first_block=False):
          blk = nn.Sequential()
          for i in range(num_residuals):
              if i == 0 and not first_block:
                  blk.add(Residual(num_channels, use_1x1conv=True, strides=2))
              else:
                  blk.add(Residual(num_channels))
          return blk

      network = nn.Sequential()
      network.add(
          nn.Conv2D(64, kernel_size=7, strides=2, padding=3),
          nn.BatchNorm(), nn.Activation('relu'),
          nn.MaxPool2D(pool_size=3, strides=2, padding=1)
      )

      network.add(
          resnet_block(64, 2, first_block=True),
          resnet_block(128, 2),
          resnet_block(256, 2),
          resnet_block(512, 2))

      network.add(nn.GlobalAvgPool2D(), nn.Dense(10))

      if restore_dir:
          helper.restore(network, restore_dir)
      else:
          network.initialize(ctx=helper.ctx, init=init.Xavier())

      return network
#+END_SRC

*** 实验结果
由于上述的网络结构正好可以处理 28*28 尺度的图像,所以采用最原始 fashion-MNIST 进行训练.

-----
#+BEGIN_CENTER
#+NAME: resnet 实验结果
#+CAPTION: 输入图像为 28*28 尺度,学习率为 0.1,训练 10 轮.最终在测试集上准确性为 86%.
[[file:assets/acc_loss/resnet-01-1-10-86-loss-acc.png]]
#+END_CENTER

** DenseNet
DenseNet[fn:11]为 ResNet 引申的一种变种.与 ResNet 的主要区别为:ResNet 为逐元素加,DenseNet 为按通道连接,所以叫做稠密链接.

DenseNet 主要构建模块由稠密块(/dense block/)和过渡层(/transition layer/)组成.前者用来定义输入和输出如何连接,后者用来控制通道数,使通道数不会过大.

*** 稠密块
DenseNet 使用 ResNet 改良版的"批量归一化,激活和卷积"结构[fn:10],一个稠密块由多个具有相同输出通道的该结构组成,并将输入数据和输出在通道维度上进行连接,对应实现如下:
#+BEGIN_SRC python
  def conv_block(num_channels):
      blk = nn.Sequential()
      blk.add(nn.BatchNorm(), nn.Activation('relu'),
              nn.Conv2D(num_channels, kernel_size=3, padding=1))
      return blk

  class DenseBlock(nn.Block):
      def __init__(self, num_convs, num_channels, **kwargs):
          super(DenseBlock ,self).__init__(**kwargs)
          self.net = nn.Sequential()
          for _ in range(num_convs):
              self.net.add(conv_block(num_channels))

      def forward(self, X):
          for blk in self.net:
              Y = blk(X)
              X = nd.concat(X, Y, dim=1)
          return X
#+END_SRC

可以看到稠密块的卷积通道数为增加的通道数,因此有时候也叫增长率.

*** 过渡层
正是由于每个稠密模块都会带来数据在通道数上的增加,所以多个稠密模块堆叠使用会带来过于复杂的模型.所以需要引入过渡层来控制模型的复杂度.主要是通过 1*1 卷积层来减少通道数量,并使用步长为 2 的均值采样层来降低数据空间尺度,从而进一步降低模型复杂度.

对应实现如下:
#+BEGIN_SRC python
  def transition_block(num_channels):
      blk = nn.Sequential()
      blk.add(nn.BatchNorm(), nn.Activation('relu'),
              nn.Conv2D(num_channels, kernel_size=1),
              nn.AvgPool2D(pool_size=2, strides=2))
      return blk
#+END_SRC


*** 网络结构
DenseNet 和 ResNet 一样,首先采用单个大卷积和最大采样层.然后跟着 4 个稠密块.每个稠密块具有 4 个卷积层,通道增长率设置为 32,所以每个稠密块增加 128 个通道.

对应网络结构实现如下:
#+BEGIN_SRC python
  def build(restore_dir):

      network = nn.Sequential()

      network.add(
          nn.Conv2D(64, kernel_size=7, strides=2, padding=3),
          nn.BatchNorm(), nn.Activation('relu'),
          nn.MaxPool2D(pool_size=3, strides=2, padding=1)
      )

      num_channels, growth_rate = 64, 32
      num_convs_in_dense_blocks = [4, 4, 4, 4]

      for i, num_convs in enumerate(num_convs_in_dense_blocks):
          network.add(DenseBlock(num_convs, growth_rate))
          num_channels += num_convs * growth_rate
          if i != len(num_convs_in_dense_blocks) - 1:
              network.add(transition_block(num_channels // 2))

      network.add(nn.GlobalAvgPool2D(), nn.Dense(10))

      if restore_dir:
          helper.restore(network, restore_dir)
      else:
          network.initialize(ctx=helper.ctx, init=init.Xavier())

      return network
#+END_SRC

p*** 实验结果
DenseNet 输入数据为 96*96 的图像,学习率设置为 0.1,训练 10 轮,结果如下:
-----
#+BEGIN_CENTER
#+NAME: DenseNet 实验结果
#+CAPTION: DenseNet 学习率设置为 0.1,训练 10 轮,最终在测试集上准确性为 92%.
[[file:assets/acc_loss/dense-xavier-01-1-10-92-loss-acc.png]]
#+END_CENTER

** TODO MobileNet
* 优化算法
优化算法主要就是在面对目标函数时,通过一定的优化策略来选择模型参数从而最小化目标函数.在深度学习中十分重要,会影响模型的收敛速度和最终模型收敛效果.所以理解常用优化算法有利于模型训练和调参.

深度学习中的绝大多数目标函数都非常复杂,也就是说直接最小化目标函数的解析解一般是不存在的,所以基本都是通过基于数值方法来迭代最小化目标函数.所以如下会介绍一些这类数值方法求解函数目标解的方法.

** 挑战
优化在深度学习中面领很多挑战,主要是面对一个非常复杂的网络时,无法保证通过基于梯度的方法一定能够找到局部最小值.主要的两个挑战为:局部最小值和鞍点.

*** 局部最小值
对于目标函数 $f(x)$, 如果 $f(x)$ 在 x 上的值比在 x 临近的其他点的值更小,那么 $f(x)$ 可能是局部最小值(/local minimum/).如果 $f(x)$ 在 x 上的值是目标函数在整个定义域上的最小值,那么 $f(x)$ 是全局最小值(/global minimum/).

例如,给定函数:
\begin{equation}
  f(x) = x * cos(\pi{x}), -1.0 \leq x \leq 2.0,
\notag
\end{equation}

对应的函数图:
-----
#+BEGIN_CENTER
[[file:assets/xcosx.png]]
#+END_CENTER

深度模型的损失函数一般具有非常多的最小值局部,基于梯度的迭代优化算法在局部最小值附近时,由于目标函数的梯度接近或者变成零,从而很容易使得优化算法无法逃出局部最小,从而使得模型无法收敛到更好的数值解.

*** 鞍点
梯度接近零和变成零的可能是由于当前解在局部最优附近造成的.其实,还有可能是在鞍点(/saddle point/)附近.

例如,给定函数:
\begin{equation}
  f(x) = x^{3}
\notag
\end{equation}

对应的函数图:
-----
#+BEGIN_CENTER
[[file:assets/x3.png]]
#+END_CENTER

在比如,函数:
\begin{equation}
  f(x,y) = x^{2} - y^{2}
\notag
\end{equation}

对应的函数图:
----
#+BEGIN_CENTER
[[file:assets/x2_y2.png]]
#+END_CENTER

在上图红点上可以看到,梯度在 x,y 方向为 0;并且目标函数在 x 轴上是局部最小值,在 y 轴上是局部最大值.

线性代数可知:
#+BEGIN_QUOTE
+ 当目标函数的 Hessian 矩阵在梯度为 0 的位置上的特征值全为正时,目标函数处在局部最小值位置.
+ 当目标函数的 Hessian 矩阵在梯度为 0 的位置上的特征值全为负时,目标函数处在局部最大值位置.
+ 当目标函数的 Hessian 矩阵在梯度为 0 的位置上的特征值有正有负时,目标函数处在鞍点位置.
#+END_QUOTE

并且可以知道,假设特征值为正和为负的概率都是 0.5,那么对于一个较大的矩阵,全部特征都为正或者负的概率为 $0.5^{k}$,所以对于深度模型来说,鞍点出现的概率要比局部最小值更为常见.

** 梯度下降
迭代数值优化算法基本都是基于梯度下降算法改进的.最为原始的梯度下降算法是通过沿着梯度的反方向移动,来降低目标函数的值.

数学上讲,假设连续可导函数 $f: \mathbb{R} \to \mathbb{R}$.根据一阶泰勒展开,可以得到在 x 点附近近似:
\begin{equation}
  f(x + \epsilon) \approx f(x) + \epsilon f'(x).
\notag
\end{equation}

其中, $f'(x)$ 为函数 f 在 x 处的梯度.存在一个常数 $\eta>0$,使得 $|\eta{f'(x)}|$ 足够小,那么可以将 $\epsilon$ 替换为 $-\etaf'(x)$,从而采用上述等式进行近似:
\begin{equation}
  f(x - \eta{f'(x)}) \approx f(x) - \eta{f'(x)}^{2}
\notag
\end{equation}

可以看到,如果 $f'(x) \ne 0$,那么 $\eta{f'(x)}^{2} > 0$ ,所以:
\begin{equation}
  f(x - \eta{f'(x)}) \leq f(x)
\notag
\end{equation}

所以可以通过 $x \gets x - \eta{f'(x)}$ 迭代 x,从而降低 f(x)目标值.所以一旦进入局部最小值附近时,导数基本为 0,会出现迭代步长过小,从而无法逃出局部最小值.

*** 学习率
上述梯度下降算法中的正数 $\eta$ 就叫做学习率.是一个较为重要的超参数,需要人工设定.如果学习率较小,则会导致 x 更新速度缓慢,从而需要更多迭代才能得到较好的解.如果使用过大的学习率,$|\eta{f'(x)}|$ 可能会过大,从而使得一阶泰勒展开公式不再成立:这样就无法保证迭代的 x 会降低 f(x) 的值.

** 动量法
目标函数的梯度代表了目标函数在自变量所在位置下降速度最快的方向,所以梯度下降也叫最速下降(/stepest descent/).每次迭代中,梯度下降只会沿着当前位置的梯度方向更新自变量,这种仅仅取决于自变量所在位置更新会带来一些问题.

例如,考虑目标函数 $f(x)=0.1x_{1}^{2}+2x_{2}^{2}$.以学习率为 0.4,对应的梯度更新轨迹如下:
-----
#+BEGIN_CENTER
#+CAPTION: 目标函数在 $x_{2}$ 方向上的梯度绝对值要大于 $x_{1}$ 方向上.因此,给定学习率,梯度下降在 $x_{2}$ 方向上的移动幅度要大于 x1 方向上.
[[file:assets/01x_2_2x_2_gradient.png]]
#+END_CENTER

为了避免在 $x_{2}$ 方向上越过目标函数最优解,那么需要降低学习率.然而,这会降低自变量在 $x_{1}$ 方向的收敛速度.

如果将学习率再设大一些,例如 $\eta=0.6$,会使得自变量在 $x_2$ 方向移动过大,从而使得无法收敛.
-----
#+BEGIN_CENTER
#+CAPTION: 学习率过大,使得在 $x_{2}$ 方向上无法收敛.
[[file:assets/01x_2_2x_2_06_gradient.png]]
#+END_CENTER

*** 动量法
动能法的提出就是为了应对梯度下降的上述的问题.动能法每次迭代的公式如下:
\begin{equation}
  \nu_{t} \gets \gamma \nu_{t-1} + \eta_{t}g_{t}, \\
  x_{t} \gets x_{t-1} - \nu_{t}.
\notag
\end{equation}

其中,动量超参数 $\gamma$ 满足 $0 \leq \gamma < 1$.当 $\gamma=0$ 时,动能法等价于小批量随机梯度下降.

采用上述的动能法优化函数梯度更新轨迹如下:
-----
#+BEGIN_CENTER
#+CAPTION: 学习率 0.4,动能参数 0.5 的动能更新轨迹.可以看到采用动能法后,在 $x_2$ 方向上的移动更加平滑,在 $x_1$ 方向上更快逼近最优解.
[[file:assets/01x_2_2x_2_04_momentum_gradient.png]]
#+END_CENTER

-----
#+BEGIN_CENTER
#+CAPTION: 学习率 0.6,动能参数 0.5 的动能更新轨迹.可以看到采用动能法即使采用较大的学习率,模型也能收敛.
[[file:assets/01x_2_2x_2_06_momentum_gradient.png]]
#+END_CENTER

*** 指数加权移动平均
为了从数学上理解动能法,先了解一下指数加权移动平均(/exponentially weighted moving average/).给定超参数 $0 \leq \gamma < 1$,当前时间步 t 的变量 $y_t$ 是上一步时间 t-1 的变量 $y_{t-1}$ 和当前时间步 x_{t}的线性组合:
\begin{equation}
  y_{t} = \gamma{y_{t_1}} + (1 - \gamma)x_{t}.
  \notag
\end{equation}

可以对 $y_{t}$ 进行展开:
\begin{aligned}
  y_{t} &= (1-\gamma)x_{t} + \gamma y_{t-1} \\
  &= (1-\gamma)x_{t} + (1-\gamma)\gamma{x_{t-1}} + \gamma^{2}y_{t-2} \\
  &= (1-\gamma)x_{t} + (1-\gamma)\gamma{x_{t-1}} + (1-\gamma)\gamma^{2}x_{t-2} + \gamma^{3}y_{t-3} \\
  ...
\notag
\end{aligned}

令 $n=1/(1-\gamma)$, 那么 $(1-1/n)^{n}=\gamma^{1/1-\gamma}$.由于:
\begin{equation}
  \lim_{n \to \infty}(1-\frac{1}{n})^{n} = exp(-1) \approx 0.3679
\notag
\end{equation}

当 $\gamma \to 1$ 时, $\gamma^{1/(1-\gamma)}=exp(-1)$.如果把 exp(-1)当作一个比较小的数,那么近似可以忽略掉比 $\gamma^{1/(1-\gamma)}$ 高阶的项.当 $\gamma=0.95$ 时: $y_{t} \approx 0.05\sum_{i=0}^{19}0.95^{i}x_{t-i}$.

所以 $y_{t}$ 可以看作是最近 $1/(1-\gamma)$ 个时间步的 $x_{t}$ 值的加权平均.

*** 指数加权移动平均理解动能法
现在,可以对动量法作变形:
\begin{equation}
  \nu_{t} \gets \gamma \nu_{t-1} + (1 - \gamma)(\frac{\eta_{t}}{1-\gamma}g_{t})
\notag
\end{equation}

由指数加权移动平均的形式可得,速度变量 $\nu_{t}$ 实际上对序列 ${\eta_{t-i}g_{t-i}/(1-\gamma):i=0,...,1/(1-\gamma)-1}$ 做了指数加权移动平均.即,动量法在每个时刻的更新近似于最近 $1/(1-\gamma)$ 个时间步的更新量做了指数加权移动平均后再除以 $1-\gamma$.所以,在某个方向上如果最近的更新量都是一个方向,那么更新速度会加快;如果更新量来回反复,那么更新速度就会变慢.

** Adagrad
在之前的优化算法中,目标函数自变量都使用同一个学习率来进行迭代更新.假设目标函数为 f,自变量为一个二维向量 $\left[x_{1},x_{2} \right]^{T}$,该向量中每一个元素在迭代时使用相同的学习率进行更新:
\begin{equation}
  x_{1} \gets x_{1} - \eta \frac{\partial{f}}{\partial{x_{1}}}, x_{2} \gets x_{2} - \eta \frac{\partial{f}}{\partial{x_{2}}}
\notag
\end{equation}

可以看到在两个自变量上,如果梯度值相差较大,那么需要选择一个合适的学习率使得更新迅速的自变量不会发散,更新缓慢的自变量不会收敛的太慢.动能法通过指数加权移动平均使得自变量的更新方向如果更为一致,则更新步长大;自变量的更新方向如果不一致,则更新步长小.

Adagrad[fn:12] 算法会根据自变量在每个维度的梯度值大小来调整各个维度上的学习率,从而避免统一的学习率难以适应所有自变量的问题.

*** Adagrad 算法
Adagrad 算法会对更新的梯度 $g_{t}$ 逐元素平方累加变量 $s_{t}$: $s_{t} \gets s_{t-1} + g_{t} \bigodot g_{t}$.其中 $\bigodot$ 为逐元素相乘.接着,目标函数自变量中每个元素的学习率通过累加元素重新调整-下:
\begin{equation}
  x_{t} \gets x_{t-1} - \frac{\eta}{\sqrt{s_{t}+\epsilon}} \bigodot g_{t}
\notag
\end{equation}

其中, $\eta$ 为学习率, $\epsilon$ 是为了维持数值稳定性而添加的常数,例如 $10^{-6}$.这样使得更新的每个自变量都具有独立的学习率.

可以看到,如果目标函数有关自变量中某个元素的偏导数一直都比较大,那么该元素的学习率下降较快;反之,如果目标函数有关自变量中某个元素的偏导数一直都比较小,那么该元素的学习率下降较慢.但是由于 $s_{t}$ 一直在累加元素平方,所以学习率是一直下降的.所以当学习率在迭代早期降的较快且当前解依然不佳的时候,Adagrad 在迭代后期会因为学习较小,使得模型无法找到一个有用的解.

** RMSProp
Adagrad 通过累加梯度逐元素的平方来作为学习率调整的分母,从而使得学习率在迭代过程中一直在降低或者不变.所以 Adagrad 算法在后期会出现学习率过低,模型无法迭代的情况.为了应对这一问题,RMSProp[fn:13] 算法做了一点改动.

RMSProp 可以看作是在 Adagrad 累加逐元素的平方过程中引入了动能法中的指数加权移动平均,从而使得学习率调整依赖于过去 t 个时间步的梯度平方,如此一来,学习率在迭代的过程中不会一直降低.
\begin{equation}
  s_{t} \gets \gamma s_{t-1} + (1-\gamma)g_{t} \bigodot g_{t}, \\
  x_{t} \gets x_{t-1} - \frac{\eta}{\sqrt{s_{t}+\epsilon}} \bigodot g_{t}.
\notag
\end{equation}

** Adadelta
除了 RMSProp 以外,Adadelta [fn:14]算法也是针对 Adagrad 在迭代后期可能较难找到有用解的问题做了改进.并且 Adadelta 没有学习率这一超参.Adadelta 算法也通过指数加权移动平均累加梯度的逐元素平方和:
\begin{equation}
  s_{t} \gets \gamma s_{t-1} + (1-\gamma)g_{t} \bigodot g_{t},
\notag
\end{equation}

Adadelta 另外维护了一个额外状态变量 $\Delta{x_{t}}$,来计算自变量的变化量:
\begin{equation}
  g_{t}' \gets \sqrt{\frac{\Delta{x_{t-1}}+\epsilon}{s_{t}+\epsilon}} \bigodot g_{t},
\notag
\end{equation}

自变量的更新公式:
\begin{equation}
  x_{t} \gets x_{t-1} - g_{t}'
\notag
\end{equation}

最后,更新 $\Delta{x}$ 来记录自变量变化量 $g'$ 安元素平方的指数加权移动平均:
\begin{equation}
  \Delta{x_{t}} \gets \gamma \Delta{x_{t-1}} + (1-\gamma)g_{t}' \bigodot g_{t}'.
\notag
\end{equation}

可以看到的,Adadelta 根 RMSProp 不同之处在于使用了 $\sqrt{\Delta{x_{t-1}}}$ 来替代学习率超参.

** Adam
Adam[fn:15] 在 RMSProp 基础上对梯度也做了指数加权移动平均:
\begin{equation}
  \nu_{t} \gets \beta_{1} \nu_{t-1} + (1 - \beta_{1})g_{t}
\notag
\end{equation}

对梯度的逐元素平方项做指数加权移动平均得到 $s_{t}$:
\begin{equation}
  s_{t} \gets \beta_{2}s_{t-1} + (1-\beta_{2})g_{t} \bigodot g_{t}.
\notag
\end{equation}

需要注意的是:在迭代开始时,例如 $\beta_{1}=0.9$时, $v_{1}=0.1g_{1}$,为了消除这种刚开始加权平均较小的问题,可以除以 $1-\beta_{1}^{t}$,从而使得过去的时间个时间步的权值之和为 1,叫做偏差修正.
\begin{equation}
  \hat{\nu}_{t} \gets \frac{\nu_{t}}{1-\beta_{1}^{t}}. \\
  \hat{s}_{t} \gets \frac{s_{t}}{1-\beta_{2}^{t}}.
\notag
\end{equation}

对应元素的更新量为:
\begin{equation}
  g_{t}' \gets \frac{\eta \hat{\nu}_{t}}{\sqrt{\hat{s}_{t}+\epsilon}}
\notag
\end{equation}

* 归一化方法 
有很多归一化方法,主要可以分为两种:
+ 对输入数据进行归一化
+ 在神经网络计算过程中网络中的激活值归一化


通常来说,对输入数据进行归一化有利于训练浅层模型.但是由于深度模型训练过程中,越靠近输出层的激活值越会出现剧烈震荡.所以对于深度模型即使输入数据已经归一化,但是网络中的激活值震荡通常会增加深度模型的训练.

所以,在神经网络计算过程中对于网络中的激活值进行归一化操作是非常有效的方法.批量归一化[fn:8]就是这种归一化方法.在模型训练的过程中,批量归一化主要是计算一个小批量上的激活值的均值和标准差来对中间激活值进行归一化操作,从而使得整个网络模型的各层的输出值更为稳定.从而使得深度模型可以得到训练或者加速训练.

** Batch Normalization
批量归一化操作在对卷积层和全链接层的操作略有不同.

*** 全链接层批量归一化
通常,批量归一化操作是在全链接层中的仿射变换和激活函数之间.设全链接层的输入为 $\mu$, 权重参数和偏差参数为 W 和 b,激活函数为 $\phi$.设批量归一化操作为 $BN$.那么,使用批量归一化的全链接层的输出为: 
\begin{equation}
  \phi{(BN(W\mu+b))}
\notag
\end{equation}

假设,仿射变换输出为 m 个样本的小批量 $\mathcal{B}={{x^{1},,...x^{m}}}$.这些是批量归一化的输入数据.对于批量 $\mathcal{B}$ 中的任意样本 $x^{i} \in R^{d}, 1 \leq i \leq m$,批量归一化层的输出同样为 d 维向量:
\begin{equation}
  y^{i} = BN(x^{i})
\notag
\end{equation}

批量归一化步骤如下:
+ =计算批量样本的均值和方差= ::
                    \begin{equation}
                      \mu_{\mathcal{B}} \gets \frac{1}{m}\sum_{i=1}^{m}x^{i}, \\
                      \sigma_{\mathcal{B}}^{2} \gets \frac{1}{m}\sum_{i=1}^{m}(x^{i}-\mu_{\mathcal{B}})^{2},
                    \notag
                    \end{equation}
+ =逐元素标准化= ::
              \begin{equation}
                \hat{x}^{i} \gets \frac{x^{i}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}}
  
              \notag
              \end{equation}
              其中, $\epsilon>0$ 为一个很小的常数,从而保证分母是大于 0 .
+ =逐元素仿射变换= :: 经过上述标准化后,又引入两个可学习参数,用来对标准化值进行仿射变换.
               \begin{equation}
                 y^{i} \gets \gamma \bigodot \hat{x}^{i} + \beta.
               \notag
               \end{equation}
               这两个参数是可学习的,如果批量归一化没有用,那么理论上模型只需要学习出 $\beta = \sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}$ 和 $\beta = \mu_{\mathcal{B}}$ 就可以抵消批量归一化操作.

*** 卷积层批量归一化
对于卷积操作来说,批量归一化操作是在卷积计算之后,激活函数之前执行.如果卷积计算输出多个通道,则需要对这些通道分别作批量归一化操作,且每个通道独立拥有仿射变换参数,且为标量.假设 m 个样本,在单个通道上,如果卷积计算输出的高和宽分别为 p 和 q.则需要对该通道中的 m*p*q 个元素同时作批量归一化.这些元素共同参与均值和方差的计算.

*** 预测时批量归一化
使用批量归一化训练时,可以将批量的大小设的大一些,从而使得批量内的样本的均值和方差的计算更为准确.将训练好的模型拿来预测的时候,希望单个样本的输出不应该取决于批量归一化所需要的随机小批量的均值和方差,一个常见的做法是通过衰减累加的形式估算整个训练集的样本均值和方差,并在预测时候使用这些值来获得稳定输出.

#+BEGIN_SRC python
  def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):

      if not autograd.is_training():  # 判断是否是预测模型
          X_hat = (X - moving_mean) / nd.sqrt(moving_var+eps)
      else:
          assert len(X.shape) in (2, 4)
          if len(X.shape) == 2:   # 全链接层
              mean = X.mean(axis=0)
              var = ((X-mean)**2).mean(axis=0)
          else:                   # 卷积
              mean = X.mean(axis=(0, 2, 3), keepdims=True)
              var = ((X-mean)**2).mean(axis=(0, 2, 3), keepdims=True)
          # 训练模式下,使用批量均值和方差执行标准化
          X_hat = (X-mean)/nd.sqrt(var + eps)
          # 更新均值和方差
          moving_mean = momentum * moving_mean + (1 - momentum) * mean
          moving_var = momentum * moving_var + (1 - momentum) * var

      Y = gamma * X_hat + beta
      return Y, moving_mean, moving_var
#+END_SRC

需要重载一个 Block 对象,保存均值和方差信息.
#+BEGIN_SRC python
  class BatchNorm(nn.Block):
      def __init__(self, num_features, num_dims, **kwargs):
          super(BatchNorm, self).__init__(**kwargs)
          if num_dims == 2:
              shape = (1, num_features)
          else:
              shape = (1, num_features, 1, 1)
          self.gamma = self.params.get("gamma", shape=shape, init=init.One())
          self.beta = self.params.get("beta", shape=shape, init=init.Zero())
          self.moving_mean = nd.zeros(shape)
          self.moving_var = nd.zeros(shape)

      def forward(self, X):
          if self.moving_mean.context != X.context:
              self.moving_mean = self.moving_mean.copyto(X.context)
              self.moving_var = self.moving_var.copyto(X.context)
          Y, self.moving_mean, self.moving_var = batch_norm(X, self.gamma.data(), self.beta.data(), self.moving_mean, self.moving_var,
                                                            eps=1e-5, momentum=0.9)
          return Y
#+END_SRC

*** 实验结果
在 LeNet 网络结构上进行批量归一化实验.

对应网络结构:
#+BEGIN_SRC python 
  def build_LeNet(restore_dir, activation):
      network = nn.Sequential()
      network.add(
          nn.Conv2D(channels=6, kernel_size=5, strides=1),
          BatchNorm(6, num_dims=4),
          nn.Activation('sigmoid'),
          nn.MaxPool2D(pool_size=2, strides=2),
          nn.Conv2D(channels=16, kernel_size=5),
          BatchNorm(16, num_dims=4),
          nn.Activation('sigmoid'),
          nn.MaxPool2D(pool_size=2, strides=2),
          nn.Dense(120, activation=activation),
          BatchNorm(120, num_dims=2),
          nn.Activation('sigmoid'),
          nn.Dense(84, activation=activation),
          BatchNorm(84, num_dims=2),
          nn.Activation('sigmoid'),
          nn.Dense(10)
      )

      if restore_dir:
          helper.restore(network, restore_dir)
      else:
          network.initialize(init=init.Xavier(), ctx=helper.ctx)

      return network
#+END_SRC

训练结果:
-----
#+BEGIN_CENTER
#+NAME: lenet batchnorm 
#+CAPTION: 学习率为 0.1,批次大小设置为 256.10 轮训练的结果,最终在测试集上准确性为 86%.
[[file:assets/acc_loss/lenet-batchnorm-01-1-10-86-loss-acc.png]]
#+END_CENTER

* 正则化

** L1/L2

** Dropout

* 数据增强

* 模型集成

* 模型压缩&剪枝

* Footnotes

[fn:15] Adam: A method for stochastic optimization

[fn:14] ADADELTA: an adaptive learning rate method

[fn:13] Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude

[fn:12] Adaptive subgradient methods for online learning and stochastic optimization

[fn:11] Densely connected convolutional networks

[fn:10] Identity mappings in deep residual networks

[fn:9] Deep residual learning for image recognition

[fn:8] Batch normalization: Accelerating deep network training by reducing internal covariate shift

[fn:7] Going deeper with convolutions

[fn:6] https://arxiv.org/abs/1312.4400

[fn:5] https://arxiv.org/abs/1506.01186

[fn:4] http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf

[fn:3] Gradient-Based Learning Applied to Document Recognition

[fn:2] https://github.com/zalandoresearch/fashion-mnist

[fn:1] https://zh.diveintodeeplearning.org/index.html


