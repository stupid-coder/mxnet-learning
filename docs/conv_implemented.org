#+TITLE: 卷积网络实验
#+AUTHOR: stupid-coder
#+EMAIL: stupid-coder
#+STARTUP: indent
#+OPTIONS: H:2 ^:nil
#+INDEX: (mxnet)

#+BEGIN_QUOTE
本文用来说明一些常见卷积网络结构的实验,从而学习相关知识点.本文中代码都是在 cpu 上运行,所以网络结构都是简化版本.
#+END_QUOTE

* 卷积神经网络
  从 1998 年 LeNet 诞生,到 2012 年 AlexNet 在 ImageNet 分类比赛上大放异彩,使得卷积神经网络得到了新生.卷积神经网络的基础思想是利用图像天然的局部相关特性,使用局部链接提高模型对局部信息的敏感度,权值共享的方式减低模型参数,采样层来增强尺度不变性.采用反向传播算法自学习卷积核,从而避免了人工构造视觉特征需要大量专业知识的弊端.

  但是从头训练一个卷积神经网络模型需要大量的工程经验和模型训练经验.本文会在一个较为简单的数据集上尝试从头开始训练一个卷积神经网络,采用不同的网络模型(不同的网络结构&不同的激活函数等),不同的训练策略(学习率&初始化策略&优化算法).并希望能够在这个较小的数据集上达到绝对过拟合(训练集上准确性基本接近 100%),最后利用数据增强和正则化方法来降低模型过拟合,并结合模型集成相关技术,最终得到一个能够达到的最好模型.并且会尝试相关模型压缩等技术.

  本文会沿着如下路线进行说明和记录:
  + =基础网络= :: 首先尝试使用较为简单的 LeNet 作为卷积神经网络示例.
  + =学习率= :: 使用不同的学习率进行训练.
  + =激活函数= :: 采用 tanh, softplus, relu 的卷积神经网络.
  + =权值初始化= :: 采用 Xvair 的权值初始化.
  + =网络结构= :: AlexNet,VGG,NiN,GoogLeNet,ResNet,DenseNet.
  + =优化算法= :: sgd,adam
  + =归一化方法= :: batch-norm,layer-norm,instance-norm,group-norm.
  + =正则化方法= :: dropout,全局均值采样
  + =数据增强= :: 尝试常见的图像数据增强方法降低模型过拟合
  + =模型集成= :: 提高最终模型表现.
  + =模型压缩&剪枝= :: 模型压缩和剪枝相关尝试.

  最近在看李牧大神的<动手学深度学习>[fn:1],所以本文代码采用 mxnet 深度学习框架实现.

  评估标准除了传统的损失值和准确率.

* 数据集
首先介绍一下数据集,采用和 mxnet 示例中使用的相同图像分类数据集 Fashion-MNIST[fn:2],该数据集要比原始的 MNIST 要复杂一点,如[[图 1]]所示:

#+BEGIN_CENTER
#+NAME: 图 1
#+CAPTION: Fashion-MNIST 一些样本展示,每 3 行一个类别.
[[file:assets/fashion-mnist-sprite.png]]
#+END_CENTER

*mxnet* 中 *gluon.data.vision* 模块已经内置了该数据集,读取方式如下:
#+BEGIN_SRC python
  def data():
      return gdata.vision.FashionMNIST(train=True), gdata.vision.FashionMNIST(train=False)


  def dataset(batch_size=64):
      train_data, test_data = data()

      transformer = gdata.vision.transforms.Compose([
          gdata.vision.transforms.ToTensor()
          ])

      train_iter = gdata.DataLoader(train_data.transform_first(transformer),
                                    batch_size=batch_size, shuffle=True)

      test_iter = gdata.DataLoader(test_data.transform_first(transformer),
                                   batch_size=batch_size, shuffle=True)


      return train_iter, test_iter
#+END_SRC

标签数据为:
| 标签 | 描述        |
|------+-------------|
|    0 | T-shirt/top |
|    1 | Trouser     |
|    2 | Pullover    |
|    3 | Dress       |
|    4 | Coat        |
|    5 | Sandal      |
|    6 | Shirt       |
|    7 | Sneaker     |
|    8 | Bag         |
|    9 | Ankle boot  |

* LeNet 卷积神经网络
  卷积神经网络一般的网络结构为: [[conv]*+[pooling]*]* + [fc]*,即通过级联多组卷积层和采样层,然后级联一个或多个全链接层来实现.本文首先实现一个最为简单的卷积神经网络 LeNet[fn:3] 作为基础网络.

  LeNet 出现较早,为第一个实用的经典卷积神经网络模型.整个网络结构如[[图 2]]所示,由 2 层卷积层和 2 个最大值采样层交替组成,最后级联 3 层全链接层作分类.整个网络都是采用 5*5 卷积核,2*2 最大值采样层, sigmoid 作为激活函数.

  #+BEGIN_CENTER
  #+NAME: 图 2
  #+CAPTION: LeNet 网络结构.
  [[file:assets/LeNet.png]]
  #+END_CENTER

** LeNet 网络结构
采用 mxnet 构建网络结构,具体实现可以参考 mxnet 官网的教程.
#+BEGIN_SRC python
  def build_LeNet():
      net = nn.Sequential()
      net.add(
          nn.Conv2D(channels=6, kernel_size=5, strides=1, activation='sigmoid'),
          nn.MaxPool2D(pool_size=2, strides=2),
          nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),
          nn.MaxPool2D(pool_size=2, strides=2),
          nn.Dense(120, activation='sigmoid'),
          nn.Dense(84, activation='sigmoid'),
          nn.Dense(10)
      )
      net.initialize()
      return net
#+END_SRC

原论文中,最后一层的全链接层为采用颈向基函数来计算前一层输出 84 神经元与 [7*12] 的位图的欧式距离来进行对应的预测的(如[[图 3]]所示):
#+BEGIN_CENTER
#+NAME: 图 3
#+CAPTION: [7*12] 的位图.
[[file:assets/RBF_bitmap.png]]
#+END_CENTER

本文并不采用这种方法,采用交叉熵作为损失函数,SGD 算法进行优化.但是保持了 LeNet 的网络结构,中间全链接层仍然为 84 神经元.

输入数据为 [28,28,1] 的灰度图像.可以通过如下代码对网络进行预初始化,并打印出每层网络输出:
#+BEGIN_SRC python
  def pre_initialize_net(net):
      X = nd.random.uniform(shape=(1, 1, 28, 28)) # mxnet 中为[batch_size,channels,height,width]
      for layer in net:
          X = layer(X)
          print(layer.name, "output shape:\t", X.shape)
#+END_SRC

输出结果:
#+BEGIN_EXAMPLE
  conv0 output shape:	 (1, 6, 24, 24)
  pool0 output shape:	 (1, 6, 12, 12)
  conv1 output shape:	 (1, 16, 8, 8)
  pool1 output shape:	 (1, 16, 4, 4)
  dense0 output shape:	 (1, 120)
  dense1 output shape:	 (1, 84)
  dense2 output shape:	 (1, 10)
#+END_EXAMPLE

整个模型参数量为:
+ conv0: 6 * (5 * 5 * 1 + 1) = 156
+ conv1: 16 * (5 * 5 * 6 + 1) = 2416
+ dense0: 120 * (16 * 4 * 4 + 1) = 30840
+ dense1: 84 * (120 + 1) = 10164
+ dense2: 10 * (84 + 1) 850

总共参数量为: 44,426.

** 模型训练
损失函数采用交叉熵损失,优化器采用随机梯度下降(/SGD/).

#+BEGIN_SRC python
  def train(net, trainer, train_iter, test_iter, loss, num_epochs=5):
      train_ls = []
      train_acc = []
      test_ls = []
      test_acc = []
      for i in range(num_epochs):
          train_ls_sum, train_acc_sum = 0, 0
          begin_clock = time.clock()

          for X, y in train_iter:
              with autograd.record():
                  y_hat = net(X)
                  l = loss(y_hat, y).mean()
              l.backward()
              trainer.step(1)
              train_ls_sum += l.asscalar()
              train_acc_sum += accuracy(y_hat, y)

          train_ls.append(train_ls_sum/len(train_iter))
          train_acc.append(train_acc_sum/len(train_iter))
          tloss, tacc = evaluate(test_iter, net, loss)
          test_ls.append(tloss)
          test_acc.append(tacc)

          end_clock = time.clock()

          print("epoch {} - train loss: {}, train accuracy: {}, test loss: {}, test_accuracy: {}, cost time:{}".format(
              i+1, train_ls[-1], train_acc[-1], test_ls[-1], test_acc[-1], end_clock-begin_clock))
      return train_ls, train_acc, test_ls, test_acc


  def main(batch_size, lr):
      net = build_LeNet()
      describe_net(net)
      train_iter, test_iter = dataset(batch_size)
      trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':lr}) # SGD 优化器
      plot_loss_and_acc(train(net, trainer, train_iter, test_iter, gloss.SoftmaxCrossEntropyLoss()))
#+END_SRC

可以看到整个训练过程会将 batch 训练的损失值和准确值进行加和平均.然后在每次 epoch 之后计算在测试集上的损失值和准确性.对应的计算代码如下:

#+BEGIN_SRC python
  def accuracy(y_hat, y):
      return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()

  def evaluate(data_iter, net, loss_fn):
      acc = 0
      loss = 0
      for X,y in data_iter:
          y_hat = net(X)
          acc += accuracy(y_hat, y)
          loss += loss_fn(y_hat, y).mean().asscalar()
      return loss / len(data_iter), acc / len(data_iter)
#+END_SRC

** 实验结果
整个网络采用均值初始化,batch_size 设置为 256.观察效果.

在设定学习率为 0.1 时候,发现损失前期基本不下降.增大训练轮数发现,在很多轮之后,损失值和准确性有了很大的变化.如[[图 4]]所示:
#+BEGIN_CENTER
#+NAME: 图 4
#+CAPTION: 均值初始化,学习率设置为 0.1,100 轮训练结果.模型收敛速度较慢,经过 70 多个 epoch 之后才开始收敛.最终测试准确性只有 70%.
[[file:assets/acc_loss/lenet-uniform-01-1-101-73-loss-acc.png]]
#+END_CENTER

两种原因影响模型训练出现这种情况:
+ 学习率设置太低,这个很容易理解,学习率较低,模型对损失函数梯度更新不敏感.
+ 权值初始化对模型收敛也有较大影响,这个在后文中会介绍为什么权值初始化对深度神经网络的训练有着巨大的影响.

* 学习率

学习率主要应用在梯度下降算法中,用来调整一次权值更新的超参数.最简单的权值更新公式:
\begin{equation}
  \theta_{new} = \theta_{old} - \lambda \frac{\partial{J(\theta)}}{\partial{\theta}}(\theta_{old})
\notag
\end{equation}

学习率一般如何设置呢? 

其实在 LeCun 的早先一篇论文<effecient backprop>[fn:4]里有简单的介绍.基本思想就是说学习率不能设置过大,会导致模型不收敛,论文给出最优学习率公式.具体可以去看论文中说明.

学习率设置过大,有可能会带来损失值增大,模型不收敛;或者使得模型无法收敛到极小,损失无法达到最小.

学习率设置过小,损失值下降过慢,网络收敛速度慢,并且容易陷入局部极小,使得模型欠拟合,无法达到全局最优.
-----
#+BEGIN_CENTER
#+NAME: 图 5
#+CAPTION: 不同学习率对损失值的影响.可以看到较好的学习率应该可以使模型损失较快下降,并且最终损失值可以降到一个合理值.
[[file:assets/learning-rate-to-loss.jpg]]
#+END_CENTER

那么在深度神经网络中学习率如何设置呢?

LeCun 的论文中提出的最优学习率就无法直接计算,因为涉及到 Hessian 矩阵的逆.在参数量庞大的深度学习模型上,这个计算代价太大.所以大多数情况下,学习率是试出来的.但是也是有一些经验可寻的.比如说,[[http://cs231n.github.io/neural-networks-3][cs231n]]就有介绍说梯度更新比例经验值为 1e-3 附近.并且也给出了一些启发式的学习率调参方法.


#+BEGIN_QUOTE
学习率在训练过程中一般也不是一成不变的,一般常规做法是在网络前几轮训练采用较大的学习率,使得网络能够尽快开始收敛;随着训练过程,对学习率进行衰减,从而避免较大学习率无法收敛到极小的问题.
#+END_QUOTE

** 周期性学习率调整
可以想到的简单方法就是使用不同学习率在模型上简单训练几个 epoch,然后观察学习率和准确性的关系,从而确定一个学习率的大体值,在后许训练中采用学习率衰减方法.

论文<Cyclical Learning Rates for Training Neural Networks>[fn:5] 中给出了一个找寻学习率上下界的方法:在前几轮训练中,以使学习率从低到高线性增加,观察对应的准确性.将准确性开始提升的对应学习率设置为下界,将准确性开始变差或者开始上下波动的时候设置为上界.从而找到学习率的上下解.

周期性学习率调整代码如下:
#+BEGIN_SRC python
  def circle_learning_rate(iter_count, base_lr, max_lr, step_size):
      cycle = math.floor( 1 + iter_count / (2 * step_size) )
      ratio = abs( iter_count / step_size - 2 * cycle + 1 )
      return base_lr + (max_lr-base_lr) * max( (1-ratio), 0 )
#+END_SRC

其中,iter_count 为训练迭代次数,base_lr 和 max_lr 为周期性调整学习率的下界和上界,step_siz 为调整周期的一半迭代次数.学习率调整的变化图如下所示:
-----
#+BEGIN_CENTER
#+NAME: 图 6
#+CAPTION: 三角周期性调整习率策略.蓝色线表示学习率在上下学习率边界调整,step_size 为一个调整周期的一般迭代次数.
[[file:assets/circle-lr.png]]
#+END_CENTER


*** 周期性学习率上下界
论文中提出了通过在多个 epoch 中迭代递增学习率,然后通过观察学习率和损失值及准确性的关系确定最优学习率区间.将 loss 开始显著下降作为 base_lr,在 loss 开始进入平缓区或者有点上升时为 max_lr.

只需要利用上述的周期学习率调整,将 step_size 设置为最大迭代次数,就可以保证在训练周期中学习率递增.

采用 lenet 网络结构,8 个 epochs 训练周期.训练结果如下:
-----
#+BEGIN_CENTER
#+NAME: 图 7
#+CAPTION: LeNet 周期学习率测试.可以确定学习率设置为 base_lr=0.015,max_lr=0.06.在 0.015 时,loss 开始下降,在 0.06 的时候 loss 开始进入平缓,有一些上升.表示学习率有点偏大了.
[[file:assets/lenet_circle_lr_test.png]]
#+END_CENTER

*** 周期性学习率训练结果
基于上述确定的学习率上下界 base_lr=0.015 和 max_lr=0.06,设置 step_size 为,采用周期性学习率调整算法进行训练 300 轮.结果如下:
-----
#+NAME: 图 8
#+CAPTION: batch_size=256, base_lr=0.015, max_lr=0.06, step_size=8*256 周期调整学习率方法.最终准确性为 78%.
[[file:assets/acc_loss/lenet-uniform-circle-1-301-78-loss-acc.png]]

效果并不好,比固定学习率 0.1 花了更长时间损失值才开始明显下降.很明显学习率设置还是太低了.后续在做实验.

** 衰减学习
在[[http://cs231n.github.io/neural-networks-3/][cs231n]]中介绍了多种递减学习率的方法,例如如下:
+ =步衰减= :: 几轮 epoch 对学习率进行衰减,例如 5 个 epcoh 衰减成 0.5,或者 10 个 epoch 衰减到 0.1.一个启发式方法是通过观察验证集上准确性一旦保持不变,那么就对学习率衰减 0.5.
+ =指数衰减= :: 具有公式 $\alpha=\alpha_{0}e^{-kt}$,其中 $\alpha_{0},k$ 为超参,t 为训练次数.
+ =1/t 衰减= :: 具有公式 $\alpha=\alpha_{0}/(1+kt)$,其中 $\alpha_{0},k$ 为超参,t 为训练次数.

*** 步衰减
初始学习率设置为 1.0,采用 10 个 epoch 衰减 0.5 的策略,训练 100 轮结果如下:
-----
#+BEGIN_CENTER
#+NAME: 图 9
#+CAPTION: 可以看到前期由于采用 1.0 学习率,在经过 10 个左右 epoch 后,损失值就开始下降.最终准确性达到 0.83.
[[file:assets/acc_loss/lenet-uniform-10-05-decay-loss-acc.png]]
#+END_CENTER

* 调整初始化策略
不管是从[[图 4]]还是从[[图 8]]都可以看到在训练的前期损失值都有一个不下将的区间,该区间的长短由学习率的大小决定;大的学习率可以很快的进入损失值下降的区间;小的学习率需要较长时间来进入损失值下降区间.直观上将该区间内,模型在一层层初始化网络权值,学习率大初始化的快.那么有没有其他方法可以加快初始化过程呢.这里就要用到一些权值初始化方法.

** Xavier 初始化策略
Glorot 在论文<Understanding the diffculty of training deep feedforward neural networks> 通过观察不同激活函数和不同初始化策略下激活值的分布提出了叫做 Xavier 的权值初始化策略.论文中采用了均值采样 $U\left[-\frac{1}{\sqrt{n}},\frac{1}{\sqrt{n}}\right]$ 作为实验初始化策略,其中 n 为输入的神经元个数.

*** sigmoid 作为激活函数
首先论文观察 4 层采用 sigmoid 作为激活函数的激活值分布:
-----
#+BEGIN_CENTER
#+NAME: 图 9
#+CAPTION: 采用 sigmoid 作为激活函数,均值初始化的 4 层神经网络激活值分布(均值和方差).可以看到高层迅速进入饱和状态,在 100epoch 之后慢慢变成非饱和状态.
[[file:assets/activation-sigmoid-4.png]]
#+END_CENTER

分析: $sigmoid(b+Wh)$ 由于结合均值随机初始化,在训练前期低层输入隐特征 h 变化可能和分类目标没有那么具有关联,所以 softmax 分类层更倾向于学习不同分类的偏置,而将 Wh 设置为 0.反向传播过程中,$\frac{\partial{L}}{\patial{h}}$ 更倾向为 0,在用 sigmoid 作为激活函数的时候,会使得激活函数陷入饱和状态,从而使得反向传播错误无法向前传播,从而使得低层网络层无法学习到有用信息.最终缓慢的,当低网络层学习到更为有用的信息后,高网络逃出饱和状态.整个网络层随着低网络层进入饱和状态而稳定.sigmoid 的网络一般性能都较差(即泛化能力不足).

*** tanh 作为激活函数
介于上述情况,由于高网络层激活值容易首先进入 0 值范围.那么采用以 0 为均值的对称激活函数可以规避陷入饱和状态.采用均值采样观察到了饱和状态出现在了第一层,然后依此传播到高层.

-----
#+BEGIN_CENTER
#+NAME: 图 10
#+CAPTION: 98 百分位数(标记)和标准差(实线+标记).可以看到第一层先进入饱和,然后第二层,第三层.
[[file:assets/activation-tanh.png]]
#+END_CENTER

*** Softsign 作为激活函数
Softsign 激活函数 $x/(1+|x|)$ 和 tanh 激活函数很相似.除了饱和状态为平滑渐进线(多项式饱和和指数).

-----
#+BEGIN_CENTER
#+NAME: 图 11
#+CAPTION: 98 百分位数(标记)和标准差(实线+标记).可以 softsign 作为激活函数时,所有层一起慢慢进入饱和状态.
[[file:assets/activation-softsign.png]]
#+END_CENTER


并且 tanh 激活函数和 softsign 激活函数训练出的模型最后的激活值分布也是不同.tanh 的激活值分布在(-1,1)和 0 值附近.而 softsign 的激活值分布在 0 点和 0 点到(-1,1)之间的区域.
-----
#+BEGIN_CENTER
#+NAME: 图 12
#+CAPTION: (上图):为 tanh 作为激活函数的模型输出的激活值分布,可以看到低层激活值更多集中在-1 和 1 的饱和区域.(下图):为 softsign 的激活函数的模型输出的激活值分布,可以看到激活值大多数集中在(-0.6,-0.8)和(0.6,0.8)之间,没有饱和,并且是非线性区域.
[[file:assets/activation-value-of-tanh-and-sfotsign.png]]
#+END_CENTER

*** 损失函数
论文通过可视化一个单层隐层的网络,tanh 作为激活值,讨论了对数似然损失函数 $-\log{P(y|x)}$ 和 二次损失函数.下图显示二次损失函数具有更多的稳定平面.
-----
#+BEGIN_CENTER
#+NAME: 图 13
#+CAPTION: 黑色为交叉熵损失,红色为二次损失.
[[file:assets/loss_function.png]]
#+END_CENTER

*** Xavier 初始化策略
通过观察初始化状态下的梯度传播过程,优化初始化策略对梯度传播效率.

假设:全链接神经网络的激活函数为对称,并且在 0 点具有单位梯度(即 $f^{'}(0)=1$).记 $z^{i}$ 为 i 网络层的激活向量; $s^{i}$ 为 i 网络层的激活函数参数向量.所以, $s^{i}=z^{i}W^{i}+b^{i}$ 和 $z_{i+1}=f(s^{i})$,定义梯度为:
\begin{equation}
  \frac{\partial{Cost}}{\partial{s_{k}^{i}}} = f'(s_{k}^{i})W_{k}^{i+1}\frac{\partial{Cost}}{\partial{s^{i+1}}} \\
  \frac{\partial{Cost}}{\partial{w_{l,k}^{i}}} = z_{l}^{i}\frac{\partial{Cost}}{\partial{s_{k}^{i}}}
\notag
\end{equation}

激活值的方差可以表示为输入数据方差和初始权重的方差线性组合(由于激活值在 0 值附近,且梯度为 1,所以 f(x)=x):
\begin{equation}
  f'(s_{k}^{i}) = 1 \\
  Var[z^{i}] = Var[x]\prod_{i'=0}^{i-1}n_{i'}Var[W^{i'}]
\notag
\end{equation}

其中 $Var[W^{i'}]$ 为第 $i'$ 网络层的初始化权重共享的方差, $n^{i'}$ 为第 i^{i} 网络层的权重数量.对于一个具有 d 层的网络来说:
\begin{aligned}
  Var[\frac{\partial{Cost}}{\partial{s^{i}}}] &= Var[\frac{\partial{Cost}}{\partial{s^{d}}}]\prod_{i'=i}^{d}n_{i'+1}Var[W^{i'}] \\
    Var\left[\frac{\partial{Cost}}{\partial{w^{i}}}\right] &= \prod_{i'=0}^{i-1}n_{i'}Var[W^{i'}]\prod_{i'=i}^{d-1}n_{i'+1}Var[W^{i'}] \\
    & \times Var[x]Var[\frac{\partial{Cost}}{\partial{s^{d}}}]
\notag
\end{aligned}

从向前传播角度看,希望激活值的方差不变,则: $\forall{(i,i')},Var[z^{i}]=Var[z^{i'}]$.

从反向传播角度看,希望对于激活前的值方差不变: $\forall{(i,i')},Var[\frac{\partial{Cost}}{\partial{s^{i}}}]=Var[\frac{\partial{Cost}}{\partial{s^{i'}}}]$

要满足上述两种约束,只需要:
\begin{equation}
  \forall{i}, n_{i}Var[W^{i}] = 1 \\
  \forall{i}, n_{i+1}Var[W^{i}] = 1 
  \notag
\end{equation}

结合起来就是: $\forall{i}, Var[W^{i}]=\frac{2}{n_{i}+n_{i+1}}$. 当所有网络层具有相同神经元则满足.

结合均值随机初始化,提出 *Xvaier* 初始化方法:
\begin{equation}
  W \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_{j}+n_{j+1}}},\frac{\sqrt{6}}{\sqrt{n_{j}+n_{j+1}}}\right]
\notag
\end{equation}

*** 实验结果

LeNet 采用 sigmoid 作为激活函数,Xavier 作为初始化策略,学习率为 0.1,10 轮训练结果如下:
-----
#+BEGIN_CENTER
#+CAPTION: 网络从 4 epoch 开始收敛(均值初始化网络需要 70 个 epoch 才开始收敛).测试集上准确性达到了 86%.
[[file:assets/acc_loss/lenet-xavier-sigmoid-01-1-101-86-loss-acc.png]]
#+END_CENTER


LeNet 采用 tanh 作为激活函数,Xavier 作为初始化策略,学习率为 0.1,10 轮训练结果如下:
-----
#+BEGIN_CENTER
#+CAPTION: 网络第一轮就开始开始收敛.最终模型在测试集上准确性达到了 89%,并有过拟合现象.
[[file:assets/acc_loss/lenet-xavier-tanh-01-1-101-89-loss-acc.png]]
#+END_CENTER


** TODO He 初始化

* 激活函数

激活函数主要是用来在深度神经网络中引入未线性变换,从而提高网络的拟合能力.激活函数在反向传播的时候,同时需要保证不要轻易出现饱和现象,从而使得梯度无法传播.不同的激活函数在不同的网络结构中使用,使用的不得当会影响网络的性能和收敛性.

** sigmoid
激活函数公式:
\begin{equation}
  sigmoid(x) = \frac{1}{1+e^{-x}} 
  \notag
\end{equation}

梯度公式:
\begin{equation}
  sigmoid'(x) = sigmoid(x)(1-sigmoid(x))
\notag
\end{equation}

激活函数图像和对应的梯度图如下:
-----
#+BEGIN_CENTER
#+CAPTION: sigmoid 的函数图像和梯度图像. 
[[file:assets/activation/sigmoid.png]]
#+END_CENTER

经过 sigmoid 激活后的值都是大于 0 的,所以均值为正数.并且可以看到梯度最大值为 0.25,所以较为容易陷入饱和状态.一般只有在特殊时候使用,例如 LSTM 网络中的各个门.

** tanh
激活函数公式:
\begin{equation}
  \tanh{(x)} = \frac{\sinh{(x)}}{\cosh{(x)})} = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
  \notag
\end{equation}

梯度公式:
\begin{equation}
  \tanh'{(x)} = 1 - \tanh^{2}{(x)}
\notag
\end{equation}

#+BEGIN_CENTER
#+CAPTION: tanh 的函数图像和梯度图像.
[[file:assets/activation/tanh.png]]
#+END_CENTER

对称的激活函数,均值为 0.优先在输入归一化的网络中.并且作为激活函数的网络收敛速度要快于 sigmoid 函数.同时看到 tanh 函数的不饱和区域很小[-2.5,+2.5],其他区域的梯度基本都接近 0 了.有时候可以通过添加一个较小的线性元素来避免这些平坦区域 $f(x)=tanh(x)+ax$.


** softsign
激活函数公式:
\begin{equation}
  signsoft(x) = \frac{x}{1-|x|}
  \notag
\end{equation}

梯度公式:
\begin{equation}
  signsoft'(x) = \frac{1}{(1+|x|)^{2}}
\notag
\end{equation}

-----
#+BEGIN_CENTER
#+CAPTION: softsign 的函数图像和梯度图像.
[[file:assets/activation/softsign.png]]
#+END_CENTER

可以看到激活函数的不饱和区域要比 tanh 要大,能够学习到更多信息.

** relu
激活函数公式:
\begin{equation}
  relu(x) = max(0, x)
  \notag
\end{equation}

梯度公式:
\begin{equation}
  relu'(x) = \begin{cases}
    1 & x > 0 \\
    0 & x < 0
    \end{cases}
  \notag
\end{equation}

-----
#+BEGIN_CENTER
#+CAPTION: relu 的函数图像和梯度图像.
[[file:assets/activation/relu.png]]
#+END_CENTER

该激活函数计算更为简单,并且在大于 0 的区域,梯度为 1,保证反向传播时,误差可以传播.

* 网络结构
LeNet 之后最为成功的卷积神经网络是 AlexNet,该网络在 ImageNet2012 比赛上大放异彩,从而使得卷积神经网络又重新走入视野之中.

** AlexNet
LeNet 提出后,卷积神经网络沉寂了很多.90 年代完全被支持向量机统治.主要原因是一方面,神经网络计算复杂,当时没有对应的 GPU 加速器,所以需要花费很久时间来训练一个现在看来并不大的神经网络.另一方面,当时也没有大量研究参数初始化和非凸优化等领域,导致复杂的神经网络也非常难以训练.

在卷积神经网络出现之前,机器视觉主要依赖于具有大量相关经验的学者研究出的人工特征,例如(SIFT,HOG),然后将这些人工构造的特征喂给机器学习来进行学习.随着卷积神经网络的大范围应用,可以预见到的是卷积神经网络就像这些有经验的学者一样通过数据来学习对应的特征表达.一层一层,从而实现最终的分类任务.由于这些特征完全是与图像和任务有关,所以提取的特征表达效率和效果都要好于人工特征.

*** 网络结构
AlexNet 的网络结构基本和 LeNet 的结构相似,不过也有一些显著区别:
+ =AlexNet 采用更深的卷积神经网络= :: 包括 8 层变换,其中包括 5 层卷积层,2 层全链接隐含层以及 1 层全链接输出层.
+ =激活函数= :: 采用 relu 作为激活函数,在正区间 relu 梯度的恒等为 1.因此不会出现因为初始化不当,使得模型无法训练.
+ =dropout= :: 采用丢弃法来控制全链接层的复杂度.
+ =数据增强= :: 采用了大量的数据增强方法,扩大数据集,从而避免过拟合.


-----
#+BEGIN_CENTER
#+NAME: AlexNet
#+CAPTION: AlexNet 网络结构.输入为 224*224 的 3 通道图像,第一层为 11*11,步长为 4 的卷积层,跟着一个 3*3,步长为 2 的最大值采样层;第二层为 5*5,步长为 1 的卷积层,跟着一个 3*3,步长为 2 的最大值采样层;第三层为 3*3,步长为 1 的卷积层;第四层为 3*3,步长为 1 的卷积层;第五层为 3*3,步长为 1 的卷积层,然后跟着一个 3*3,步长为 2 的最大值采样层.随后是两层 4096 全链接隐含层.最后为 1000 路 softmax 层.
[[file:assets/AlexNet.png]]
#+END_CENTER

对应代码如下:
#+BEGIN_SRC python
  def build_AlexNet(restore_dir):
      network = nn.Sequential()
      network.add(
          nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),
          nn.MaxPool2D(pool_size=3, strides=2),
          nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),
          nn.MaxPool2D(pool_size=3, strides=2),
          nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
          nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
          nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),
          nn.MaxPool2D(pool_size=3, strides=2),
          nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
          nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
          nn.Dense(10)
      )

      if restore_dir:
          helper.restore(network, restore_dir)
      else:
          network.initialize(ctx=helper.ctx, init=init.Xavier())

      return network
#+END_SRC

原始论文中说输入的图像为 224*224 的,但是其实如果输入为 227*227,整个网络结构和输出的特征图尺度就对了,结果如下:
#+BEGIN_EXAMPLE
INFO:helper:conv0 - output shape:(1L, 96L, 55L, 55L)
INFO:helper:pool0 - output shape:(1L, 96L, 27L, 27L)
INFO:helper:conv1 - output shape:(1L, 256L, 27L, 27L)
INFO:helper:pool1 - output shape:(1L, 256L, 13L, 13L)
INFO:helper:conv2 - output shape:(1L, 384L, 13L, 13L)
INFO:helper:conv3 - output shape:(1L, 384L, 13L, 13L)
INFO:helper:conv4 - output shape:(1L, 256L, 13L, 13L)
INFO:helper:pool2 - output shape:(1L, 256L, 6L, 6L)
INFO:helper:dense0 - output shape:(1L, 4096L)
INFO:helper:dropout0 - output shape:(1L, 4096L)
INFO:helper:dense1 - output shape:(1L, 4096L)
INFO:helper:dropout1 - output shape:(1L, 4096L)
INFO:helper:dense2 - output shape:(1L, 10L)
#+END_EXAMPLE

整个网络的参数量:
| 网络层 | 参数量                    |
| conv0  | 96*(11*11*3+1)=34944      |
| conv1  | 256*(5*5*96+1)=614656     |
| conv2  | 384*(3*3*256+1)=885120    |
| conv3  | 384*(3*3*384+1)=1327488   |
| conv4  | 256*(3*3*384+1)=884992    |
| dense0 | 4096*(256*6*6+1)=37752832 |
| dense1 | 4096*(4096+1)=16781312    |
| dense2 | 10*(4096+1)=40970         |
| total  | 58,322,314=58M            |

*** 实验结果
以 batch_size=256, learning_rate=0.1, xavier 作为初始化策略.结果如下:
-----
#+BEGIN_CENTER
#+NAME: AlexNet 试验结果
#+CAPTION: 实验参数:batch_size=256,learning_rate=0.1, xavier 初始化策略.
[[file:assets/acc_loss/alexnet-xavier-01-1-101-93-acc-loss.png]]
#+END_CENTER

可以看到虽然采用了 dropout 方法来抑制过拟合,但是由于该网络结构是为了 ImageNet 这种大数据研发的,所以在 Fashion-MNIST 上还是过于复杂了.

学习率还是应该再降低一些.后续可以做一下实验.

** VGGNet
VGGNet 为 ImageNet 2014 年的冠军网络.VGGNet 的网络改进就是加深网络层数来增强网络的非线性拟合能力,并且重复使用小尺度卷积提取体征.

*** 网络结构
VGGNet 相对于 AlexNet 进行了如下的改动:
+ 通过堆叠多个小感受野的 3*3 卷积核来获取大感受野(例如 2 层 3*3 卷积层的感受野为 5,但是额外增加了非线性能力;并且 2 层 3*3 卷积层的参数数量只有 $2*(3^{2}C^{2})=18C^{2}$,5*5 卷积层参数数量为 $5*5C^{2}=25C^{2}$).并且卷积核的步长都是 1,增加填充 1,保证卷积的空间尺度不变.
+ 引入了 1*1 卷积核,来增加通道之间的非线性变换.
+ 只采用 2*2 采样尺度,步长为 2 的最大值采样层.


-----
#+BEGIN_CENTER
#+NAME: VGGNet 网络结构
#+CAPTION: VGGNet 网络结构,根据不同卷积配置总共有 6 种 VGGNet(A-E)
[[file:assets/VGGNet.png]]
#+END_CENTER

可以看到只采用了 3*3 卷积层,每组卷积层(最大值采样层之间的卷积层)具有相同的通道数量.

参数数量:
| Network              | A,A-LRN |   B |   C |   D |   E |
| Num of parameters(M) |     133 | 133 | 134 | 138 | 144 |

对应代码如下:
#+BEGIN_SRC python
  def build_VGGNet(restore_dir):

      def vgg_block(num_convs, num_channels):
          blk = nn.Sequential()
          for _ in range(num_convs):
              blk.add(nn.Conv2D(num_channels, kernel_size=3,
                                padding=1, activation='relu'))
          blk.add(nn.MaxPool2D(pool_size=2, strides=2))
          return blk

      network = nn.Sequential()

      conv_arch = ((1,64), (1, 128), (2, 256), (2, 512), (2, 512))  # VGG11

      for (num_convs, num_channels) in conv_arch:
          net.add(vgg_block(num_convs, num_channels))

      net.add(nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
              nn.Dense(4096, activation='relu'), nn.Dropout(0.5),
              nn.Dense(10))

      if restore_dir:
          helper.restore(network, restore_dir)
      else:
          network.initialize(ctx=helper.ctx, init=init.Xavier())

      return network
#+END_SRC

*** 实验结果



** Inception

** ResNet

** DenseNet

** MobileNet

* 优化算法

** Adam

* 归一化方法 

* 正则化

** L1/L2

** Dropout


* 数据增强

* 模型集成

* 模型压缩&剪枝

* Footnotes

[fn:5] https://arxiv.org/abs/1506.01186

[fn:4] http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf

[fn:3] Gradient-Based Learning Applied to Document Recognition

[fn:2] https://github.com/zalandoresearch/fashion-mnist

[fn:1] https://zh.diveintodeeplearning.org/index.html


