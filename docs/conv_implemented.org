#+TITLE: 卷积网络实验
#+AUTHOR: stupid-coder
#+EMAIL: stupid-coder
#+STARTUP: indent
#+OPTIONS: H:2 ^:nil
#+INDEX: (mxnet)

#+BEGIN_QUOTE
本文用来说明一些常见卷积网络结构的实验,从而学习相关知识点.本文中代码都是在 cpu 上运行,所以网络结构都是简化版本.
#+END_QUOTE

* 卷积神经网络
  从 1998 年 LeNet 诞生,到 2012 年 AlexNet 在 ImageNet 分类比赛上大放异彩,使得卷积神经网络得到了新生.卷积神经网络的基础思想是利用图像天然的局部相关特性,使用局部链接提高模型对局部信息的敏感度,权值共享的方式减低模型参数,采样层来增强尺度不变性.采用反向传播算法自学习卷积核,从而避免了人工构造视觉特征需要大量专业知识的弊端.

  但是从头训练一个卷积神经网络模型需要大量的工程经验和模型训练经验.本文会在一个较为简单的数据集上尝试从头开始训练一个卷积神经网络,采用不同的网络模型(不同的网络结构&不同的激活函数等),不同的训练策略(学习率&初始化策略&优化算法).并希望能够在这个较小的数据集上达到绝对过拟合(训练集上准确性基本接近 100%),最后利用数据增强和正则化方法来降低模型过拟合,并结合模型集成相关技术,最终得到一个能够达到的最好模型.并且会尝试相关模型压缩等技术.

  本文会沿着如下路线进行说明和记录:
  + =基础网络= :: 首先尝试使用较为简单的 LeNet 作为卷积神经网络示例.
  + =学习率= :: 使用不同的学习率进行训练.
  + =激活函数= :: 采用 tanh, softplus, relu 的卷积神经网络.
  + =权值初始化= :: 采用 Xvair 的权值初始化.
  + =网络结构= :: AlexNet,VGG,NiN,GoogLeNet,ResNet,DenseNet.
  + =优化算法= :: sgd,adam
  + =归一化方法= :: batch-norm,layer-norm,instance-norm,group-norm.
  + =正则化方法= :: dropout,全局均值采样
  + =数据增强= :: 尝试常见的图像数据增强方法降低模型过拟合
  + =模型集成= :: 提高最终模型表现.
  + =模型压缩&剪枝= :: 模型压缩和剪枝相关尝试.

  最近在看李牧大神的<动手学深度学习>[fn:1],所以本文代码采用 mxnet 深度学习框架实现.

  评估标准除了传统的损失值和准确率.

* 数据集
首先介绍一下数据集,采用和 mxnet 示例中使用的相同图像分类数据集 Fashion-MNIST[fn:2],该数据集要比原始的 MNIST 要复杂一点,如[[图 1]]所示:

#+BEGIN_CENTER
#+NAME: 图 1
#+CAPTION: Fashion-MNIST 一些样本展示,每 3 行一个类别.
[[file:assets/fashion-mnist-sprite.png]]
#+END_CENTER

*mxnet* 中 *gluon.data.vision* 模块已经内置了该数据集,读取方式如下:
#+BEGIN_SRC python
  def data():
      return gdata.vision.FashionMNIST(train=True), gdata.vision.FashionMNIST(train=False)


  def dataset(batch_size=64):
      train_data, test_data = data()

      transformer = gdata.vision.transforms.Compose([
          gdata.vision.transforms.ToTensor()
          ])

      train_iter = gdata.DataLoader(train_data.transform_first(transformer),
                                    batch_size=batch_size, shuffle=True)

      test_iter = gdata.DataLoader(test_data.transform_first(transformer),
                                   batch_size=batch_size, shuffle=True)


      return train_iter, test_iter
#+END_SRC

标签数据为:
| 标签 | 描述        |
|------+-------------|
|    0 | T-shirt/top |
|    1 | Trouser     |
|    2 | Pullover    |
|    3 | Dress       |
|    4 | Coat        |
|    5 | Sandal      |
|    6 | Shirt       |
|    7 | Sneaker     |
|    8 | Bag         |
|    9 | Ankle boot  |

* LeNet 卷积神经网络
  卷积神经网络一般的网络结构为: [[conv]*+[pooling]*]* + [fc]*,即通过级联多组卷积层和采样层,然后级联一个或多个全链接层来实现.本文首先实现一个最为简单的卷积神经网络 LeNet[fn:3] 作为基础网络.

  LeNet 出现较早,为第一个实用的经典卷积神经网络模型.整个网络结构如[[图 2]]所示,由 2 层卷积层和 2 个最大值采样层交替组成,最后级联 3 层全链接层作分类.整个网络都是采用 5*5 卷积核,2*2 最大值采样层, sigmoid 作为激活函数.

  #+BEGIN_CENTER
  #+NAME: 图 2
  #+CAPTION: LeNet 网络结构.
  [[file:assets/LeNet.png]]
  #+END_CENTER

** LeNet 网络结构
采用 mxnet 构建网络结构,具体实现可以参考 mxnet 官网的教程.
#+BEGIN_SRC python
  def build_LeNet():
      net = nn.Sequential()
      net.add(
          nn.Conv2D(channels=6, kernel_size=5, strides=1, activation='sigmoid'),
          nn.MaxPool2D(pool_size=2, strides=2),
          nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),
          nn.MaxPool2D(pool_size=2, strides=2),
          nn.Dense(120, activation='sigmoid'),
          nn.Dense(84, activation='sigmoid'),
          nn.Dense(10)
      )
      net.initialize()
      return net
#+END_SRC

原论文中,最后一层的全链接层为采用颈向基函数来计算前一层输出 84 神经元与 [7*12] 的位图的欧式距离来进行对应的预测的(如[[图 3]]所示):
#+BEGIN_CENTER
#+NAME: 图 3
#+CAPTION: [7*12] 的位图.
[[file:assets/RBF_bitmap.png]]
#+END_CENTER

本文并不采用这种方法,采用交叉熵作为损失函数,SGD 算法进行优化.但是保持了 LeNet 的网络结构,中间全链接层仍然为 84 神经元.

输入数据为 [28,28,1] 的灰度图像.可以通过如下代码对网络进行预初始化,并打印出每层网络输出:
#+BEGIN_SRC python
  def pre_initialize_net(net):
      X = nd.random.uniform(shape=(1, 1, 28, 28)) # mxnet 中为[batch_size,channels,height,width]
      for layer in net:
          X = layer(X)
          print(layer.name, "output shape:\t", X.shape)
#+END_SRC

输出结果:
#+BEGIN_EXAMPLE
  conv0 output shape:	 (1, 6, 24, 24)
  pool0 output shape:	 (1, 6, 12, 12)
  conv1 output shape:	 (1, 16, 8, 8)
  pool1 output shape:	 (1, 16, 4, 4)
  dense0 output shape:	 (1, 120)
  dense1 output shape:	 (1, 84)
  dense2 output shape:	 (1, 10)
#+END_EXAMPLE

整个模型参数量为:
+ conv0: 6 * (5 * 5 * 1 + 1) = 156
+ conv1: 16 * (5 * 5 * 6 + 1) = 2416
+ dense0: 120 * (16 * 4 * 4 + 1) = 30840
+ dense1: 84 * (120 + 1) = 10164
+ dense2: 10 * (84 + 1) 850

总共参数量为: 44,426.

** 模型训练
损失函数采用交叉熵损失,优化器采用随机梯度下降(/SGD/).

#+BEGIN_SRC python
  def train(net, trainer, train_iter, test_iter, loss, num_epochs=5):
      train_ls = []
      train_acc = []
      test_ls = []
      test_acc = []
      for i in range(num_epochs):
          train_ls_sum, train_acc_sum = 0, 0
          begin_clock = time.clock()

          for X, y in train_iter:
              with autograd.record():
                  y_hat = net(X)
                  l = loss(y_hat, y).mean()
              l.backward()
              trainer.step(1)
              train_ls_sum += l.asscalar()
              train_acc_sum += accuracy(y_hat, y)

          train_ls.append(train_ls_sum/len(train_iter))
          train_acc.append(train_acc_sum/len(train_iter))
          tloss, tacc = evaluate(test_iter, net, loss)
          test_ls.append(tloss)
          test_acc.append(tacc)

          end_clock = time.clock()

          print("epoch {} - train loss: {}, train accuracy: {}, test loss: {}, test_accuracy: {}, cost time:{}".format(
              i+1, train_ls[-1], train_acc[-1], test_ls[-1], test_acc[-1], end_clock-begin_clock))
      return train_ls, train_acc, test_ls, test_acc


  def main(batch_size, lr):
      net = build_LeNet()
      describe_net(net)
      train_iter, test_iter = dataset(batch_size)
      trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':lr}) # SGD 优化器
      plot_loss_and_acc(train(net, trainer, train_iter, test_iter, gloss.SoftmaxCrossEntropyLoss()))
#+END_SRC

可以看到整个训练过程会将 batch 训练的损失值和准确值进行加和平均.然后在每次 epoch 之后计算在测试集上的损失值和准确性.对应的计算代码如下:

#+BEGIN_SRC python
  def accuracy(y_hat, y):
      return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()

  def evaluate(data_iter, net, loss_fn):
      acc = 0
      loss = 0
      for X,y in data_iter:
          y_hat = net(X)
          acc += accuracy(y_hat, y)
          loss += loss_fn(y_hat, y).mean().asscalar()
      return loss / len(data_iter), acc / len(data_iter)
#+END_SRC

** 实验结果
整个网络采用均值初始化,batch_size 设置为 256.观察效果.

在设定学习率为 0.1 时候,发现损失前期基本不下降.增大训练轮数发现,在很多轮之后,损失值和准确性有了很大的变化.如[[图 4]]所示:
#+BEGIN_CENTER
#+NAME: 图 4
#+CAPTION: 均值初始化,学习率设置为 0.1,100 轮训练结果.模型收敛速度较慢,经过 70 多个 epoch 之后才开始收敛.最终测试准确性只有 70%.
[[file:assets/acc_loss/lenet-uniform-01-1-101-73-loss-acc.png]]
#+END_CENTER

两种原因影响模型训练出现这种情况:
+ 学习率设置太低,这个很容易理解,学习率较低,模型对损失函数梯度更新不敏感.
+ 权值初始化对模型收敛也有较大影响,这个在后文中会介绍为什么权值初始化对深度神经网络的训练有着巨大的影响.

* 学习率

学习率主要应用在梯度下降算法中,用来调整一次权值更新的超参数.最简单的权值更新公式:
\begin{equation}
  \theta_{new} = \theta_{old} - \lambda \frac{\partial{J(\theta)}}{\partial{\theta}}(\theta_{old})
\notag
\end{equation}

学习率一般如何设置呢? 

其实在 LeCun 的早先一篇论文<effecient backprop>[fn:4]里有简单的介绍.基本思想就是说学习率不能设置过大,会导致模型不收敛,论文给出最优学习率公式.具体可以去看论文中说明.

学习率设置过大,有可能会带来损失值增大,模型不收敛;或者使得模型无法收敛到极小,损失无法达到最小.

学习率设置过小,损失值下降过慢,网络收敛速度慢,并且容易陷入局部极小,使得模型欠拟合,无法达到全局最优.
-----
#+BEGIN_CENTER
#+NAME: 图 5
#+CAPTION: 不同学习率对损失值的影响.可以看到较好的学习率应该可以使模型损失较快下降,并且最终损失值可以降到一个合理值.
[[file:assets/learning-rate-to-loss.jpg]]
#+END_CENTER

那么在深度神经网络中学习率如何设置呢?

LeCun 的论文中提出的最优学习率就无法直接计算,因为涉及到 Hessian 矩阵的逆.在参数量庞大的深度学习模型上,这个计算代价太大.所以大多数情况下,学习率是试出来的.但是也是有一些经验可寻的.比如说,[[http://cs231n.github.io/neural-networks-3][cs231n]]就有介绍说梯度更新比例经验值为 1e-3 附近.并且也给出了一些启发式的学习率调参方法.


#+BEGIN_QUOTE
学习率在训练过程中一般也不是一成不变的,一般常规做法是在网络前几轮训练采用较大的学习率,使得网络能够尽快开始收敛;随着训练过程,对学习率进行衰减,从而避免较大学习率无法收敛到极小的问题.
#+END_QUOTE

** 周期性学习率调整
可以想到的简单方法就是使用不同学习率在模型上简单训练几个 epoch,然后观察学习率和准确性的关系,从而确定一个学习率的大体值,在后许训练中采用学习率衰减方法.

论文<Cyclical Learning Rates for Training Neural Networks>[fn:5] 中给出了一个找寻学习率上下界的方法:在前几轮训练中,以使学习率从低到高线性增加,观察对应的准确性.将准确性开始提升的对应学习率设置为下界,将准确性开始变差或者开始上下波动的时候设置为上界.从而找到学习率的上下解.

周期性学习率调整代码如下:
#+BEGIN_SRC python
  def circle_learning_rate(iter_count, base_lr, max_lr, step_size):
      cycle = math.floor( 1 + iter_count / (2 * step_size) )
      ratio = abs( iter_count / step_size - 2 * cycle + 1 )
      return base_lr + (max_lr-base_lr) * max( (1-ratio), 0 )
#+END_SRC

其中,iter_count 为训练迭代次数,base_lr 和 max_lr 为周期性调整学习率的下界和上界,step_siz 为调整周期的一半迭代次数.学习率调整的变化图如下所示:
-----
#+BEGIN_CENTER
#+NAME: 图 6
#+CAPTION: 三角周期性调整习率策略.蓝色线表示学习率在上下学习率边界调整,step_size 为一个调整周期的一般迭代次数.
[[file:assets/circle-lr.png]]
#+END_CENTER


*** 周期性学习率上下界
论文中提出了通过在多个 epoch 中迭代递增学习率,然后通过观察学习率和损失值及准确性的关系确定最优学习率区间.将 loss 开始显著下降作为 base_lr,在 loss 开始进入平缓区或者有点上升时为 max_lr.

只需要利用上述的周期学习率调整,将 step_size 设置为最大迭代次数,就可以保证在训练周期中学习率递增.

采用 lenet 网络结构,8 个 epochs 训练周期.训练结果如下:
-----
#+BEGIN_CENTER
#+NAME: 图 7
#+CAPTION: LeNet 周期学习率测试.可以确定学习率设置为 base_lr=0.015,max_lr=0.06.在 0.015 时,loss 开始下降,在 0.06 的时候 loss 开始进入平缓,有一些上升.表示学习率有点偏大了.
[[file:assets/lenet_circle_lr_test.png]]
#+END_CENTER

*** 周期性学习率训练结果
基于上述确定的学习率上下界 base_lr=0.015 和 max_lr=0.06,设置 step_size 为,采用周期性学习率调整算法进行训练 300 轮.结果如下:
-----
#+NAME: 图 8
#+CAPTION:


** 衰减学习
在[[http://cs231n.github.io/neural-networks-3/][cs231n]]中介绍了多种递减学习率的方法,例如如下:
+ =步衰减= :: 几轮 epoch 对学习率进行衰减,例如 5 个 epcoh 衰减成 0.5,或者 10 个 epoch 衰减到 0.1.一个启发式方法是通过观察验证集上准确性一旦保持不变,那么就对学习率衰减 0.5.
+ =指数衰减= :: 具有公式 $\alpha=\alpha_{0}e^{-kt}$,其中 $\alpha_{0},k$ 为超参,t 为训练次数.
+ =1/t 衰减= :: 具有公式 $\alpha=\alpha_{0}/(1+kt)$,其中 $\alpha_{0},k$ 为超参,t 为训练次数.


* 调整初始化策略

** Xvair 初始化策略

** He 初始化

* 激活函数


** tanh

** softplus

** relu


* 网络结构

** AlexNet

** VGGNet

** Inception

** ResNet

** DenseNet

** MobileNet

* 优化算法

** Adam


* 归一化方法 

* 正则化

** L1/L2

** Dropout



* 数据增强

* 模型集成

* 模型压缩&剪枝

* Footnotes

[fn:5] https://arxiv.org/abs/1506.01186

[fn:4] http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf

[fn:3] Gradient-Based Learning Applied to Document Recognition

[fn:2] https://github.com/zalandoresearch/fashion-mnist

[fn:1] https://zh.diveintodeeplearning.org/index.html


